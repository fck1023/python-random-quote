{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 2342537,
          "sourceType": "datasetVersion",
          "datasetId": 1412891
        }
      ],
      "dockerImageVersionId": 30636,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fck1023/python-random-quote/blob/master/Generate_Amazon_Book_Reviews_with_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "# import kagglehub\n",
        "# cynthiarempel_amazon_us_customer_reviews_dataset_path = kagglehub.dataset_download('cynthiarempel/amazon-us-customer-reviews-dataset')\n",
        "\n",
        "# print('Data source import complete.')\n",
        "\n",
        "# --- 步驟一：掛接雲端硬碟 ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- 步驟二：從雲端硬碟複製 \"壓縮檔\" 到 Colab 本地 ---\n",
        "# 假設您的原始路徑是正確的\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/Generate Amazon Book Review with transfomer/amazon_reviews_us_Books_v1_02.tsv.zip\" /content\n",
        "\n",
        "# --- 步驟三：解壓縮本地的 ZIP 檔 (這是您缺少的步驟) ---\n",
        "!unzip /content/amazon_reviews_us_Books_v1_02.tsv.zip -d /content/\n",
        "\n",
        "# --- 步驟四：用 Pandas 讀取 \"解壓縮後\" 的檔案 ---\n",
        "import pandas as pd\n",
        "\n",
        "# 注意：檔名是 .tsv，代表用 Tab 分隔\n",
        "# 我們從本地的 /content 路徑讀取，速度才會快\n",
        "file_to_read = '/content/amazon_reviews_us_Books_v1_02.tsv'\n",
        "\n",
        "# 使用 read_csv 讀取 tsv 檔時，需要指定分隔符為 '\\t'\n",
        "df = pd.read_csv(file_to_read, sep='\\t', on_bad_lines='skip') # 加上 on_bad_lines 以防檔案格式問題\n",
        "\n",
        "print(\"資料讀取成功！\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "bQ-B1ZwKXsCY",
        "outputId": "c09e6b1f-950a-4e79-8010-ebd853b6d071",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Archive:  /content/amazon_reviews_us_Books_v1_02.tsv.zip\n",
            "  inflating: /content/amazon_reviews_us_Books_v1_02.tsv  \n",
            "資料讀取成功！\n",
            "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
            "0          US     12076615   RQ58W7SMO911M  0385730586       122662979   \n",
            "1          US     12703090    RF6IUKMGL8SF  0811828964        56191234   \n",
            "2          US     12257412  R1DOSHH6AI622S  1844161560       253182049   \n",
            "3          US     50732546   RATOTLA3OF70O  0373836635       348672532   \n",
            "4          US     51964897  R1TNWRKIVHVYOV  0262181533       598678717   \n",
            "\n",
            "                                                    product_title  \\\n",
            "0                      Sisterhood of the Traveling Pants (Book 1)   \n",
            "1                   The Bad Girl's Guide to Getting What You Want   \n",
            "2                          Eisenhorn (A Warhammer 40,000 Omnibus)   \n",
            "3                                 Colby Conspiracy (Colby Agency)   \n",
            "4  The Psychology of Proof: Deductive Reasoning in Human Thinking   \n",
            "\n",
            "  product_category  star_rating  helpful_votes  total_votes vine  \\\n",
            "0            Books          4.0            2.0          3.0    N   \n",
            "1            Books          3.0            5.0          5.0    N   \n",
            "2            Books          4.0            1.0         22.0    N   \n",
            "3            Books          5.0            2.0          2.0    N   \n",
            "4            Books          4.0            0.0          2.0    N   \n",
            "\n",
            "  verified_purchase                        review_headline  \\\n",
            "0                 N  this book was a great learning novel!   \n",
            "1                 N                              Fun Fluff   \n",
            "2                 N                    this isn't a review   \n",
            "3                 N              fine author on her A-game   \n",
            "4                 N          Execellent cursor examination   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            review_body  \\\n",
            "0                                                                                                                                                                                                                                                                              this boook was a great one that you could learn from. it not only teaches the imponrtance of family and their values but it also deals with basic issues that teens and some kids even deal with.  this book is about 4 best friends who are for the first time in their lives spending their summer apart. one day they are all in one of the girls rooms and finds a pair of pants that were tucked away in her closet.  once all four  of them try them on they realize that there is really something special about these pants.  seeming as how all 4 girls are differnt shapes and sizes and somehow the pants fit all of them,  they realize that these pants were the start of something special.  immediatley following they decided to make up certian rules abut the pants such as you must write the best thing u did while wearing the pants over your summer on the right leg and also some silly things such as to \\\\\"never pick yuor nose while wearing the pants.\\\\\"  this book follows the girls throuh their summers in differnt places of the world and through all of the different obstacles that life takes them through. it can really teach you alot not only about what is going on around you but most imporntantly about yuorself.  i would give this book 4 stars and would reccommend it to anyone who seems the slihgtest bit interested.   \n",
            "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        If you are looking for something to stimulate your brain, this isn't it.  However, if you are just looking for a good laugh, you'll enjoy The Bad Girl's Guide.  It's funny and light, and definitely a good way to pass a little bit of time.   \n",
            "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              never read it-a young relative idicated he liked it and somehow my name popped upon this--no more to say   \n",
            "3  Though she is honored to be Chicago Woman of the Year, Victoria Colby-Camp is more euphoric over the mental improvement that her son Jim has shown recently especially since he and Tasha North fell in love.  Jim was snatched almost twenty years ago when he was seven and turned into the killing Seth whose goal was to murder Victoria for abandoning him.  However, her elation would turn to despair if she knew Seth resurfaced and started to rape a pregnant Tasha.<br /><br />Former military strategist Daniel Marks is in town complements of the Colby Agency that wants to hire him.  Also in Chicago is Emily Hastings whose father a veteran homicide detective was murdered.  She finds letters linking her dad to Victoria, the woman's long ago murdered first husband James, and her dad's first partner Marelyn Rutland that confuses her.  Soon she will meet Daniel and they will be embroiled in the COLBY CONSPIRACY that goes back almost two decades ago.<br /><br />Though the subplots can become confusing at first, once the audience comprehends how this complex superb suspense thriller starts to come together, they will want more Colby Agency tales; (see FILES FROM THE COLBY AGENCY: THE BODYGUARD'S BABY PROTECTIVE CUSTODY).  The ensemble cast is solid as fans will feel with Victoria who has overcome so much tragedy, hope Jim \\\\\"defeats\\\\\" Seth with Tasha at his side, and root for Daniel and Emily to make it while wondering what really happened two decades ago.  A final twist marks a strong Webb of deceit tale that showcases a fine author on her A-game.<br /><br />Harriet Klausner   \n",
            "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Review based on a cursory examination by University of Phoenix students in Philosophy 251.  <br />We have found the book to be well organized, and detailed. The structure placed the information in an easy to read and presentable manner. <br />The use of strong and sound proofs by experiment and examples supported the overall logic of reasoning in a clear and concise manner. <br />We have found the book to be well thought out and laid out so the reader can learn and follow what is being explained. <br />   \n",
            "\n",
            "  review_date  \n",
            "0  2005-10-14  \n",
            "1  2005-10-14  \n",
            "2  2005-10-14  \n",
            "3  2005-10-14  \n",
            "4  2005-10-14  \n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Fake Amazon Book Reviews with Transformers\n",
        "\n",
        "![notebook-horizontal-hero.jpg](attachment:4b313cb1-881b-49ce-8458-a8b4f6d0635a.jpg)"
      ],
      "metadata": {
        "id": "AwJBs-lQXsCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# It all started with a phone call..\n",
        "\n",
        "> \"Hey Joel, we've got a problem with fake book reviews at Amazon, and I was hoping you can help.\"\n",
        "\n",
        "Sure, Jeff.  What's the problem?\n",
        "\n",
        "> \"Fake reviews are rampant on The Everything Store (you might know this as just Amazon.com), which of course reduces trust on the platform.\"\n",
        "\n",
        "Have you considered training a machine learning model to identify and flag fake reviews?\n",
        "\n",
        "> \"Of course, but the problem is, we we don't have the training data we need.  We'd need hundreds of thousands.. maybe millions, even, of fake reviews to train a reliable classifier.\"\n",
        "\n",
        "What about using Amazon's own Mechanical Turk?\n",
        "\n",
        "![8ean3d.jpg](attachment:7aebeaba-5a3a-4007-846f-8f278c7fc67b.jpg)\n",
        "\n",
        "Uh.. haha, just kidding!  Terrible idea, what was I thinking.  If you're anything like other clients I've worked with, we need this \"like, yesterday.\"  So no time for mere meatbags to write that crap.  Plus, why pay people when we can automate this?\n",
        "\n",
        "> \"My thoughts exactly.  What other ideas do you have?  This is why we pay consultants like you the big bucks!\"\n",
        "\n",
        "Umm, I haven't been paid anything ye..\n",
        "\n",
        "> \"Don't worry about that.  Do a great job with this, and maybe we'll even hire you for an AI/ML Engineering role at Amazon\"\n",
        "\n",
        "I'd love that.  Well, we could use a generative language model and fine-tune it on real Amazon book reviews, thus training it to learn to generate real-sounding reviews.  That would create our synthetic dataset we could then further use to train a classifier.\n",
        "\n",
        "> \"And it'd just put out new fake reviews, just like, abracadabra?\"\n",
        "\n",
        "Abracadabra.\n",
        "\n",
        "> \"If you could do that for me, I'd definitely reach out to Jassy and see what I could do about that engineering role, uh, situation.\"\n",
        "\n",
        "That'd be great!  So, we'll want to use a Transformer-based model for this, trained as a Causal Language Model.  Is the deliverable for this assignment just the dataset, or do you need me to write that up as prose?  ;)\n",
        "\n",
        "![8eao9r.jpg](attachment:a225d0e9-0f35-4992-87fa-2877d2895eef.jpg)\n",
        "\n",
        "Never mind, I'm on it!\n",
        "\n",
        ">  &lt;Click&gt;"
      ],
      "metadata": {
        "id": "G8iBn1ACXsCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-info\">\n",
        "  <strong>This should be obvious to everyone (though you never know these days): this intro is just for fun, and of course I've never actually spoken to Jeff Bezos in my life.  I have read <i>The Everything Store</i> (the biography about Jeff Bezos by Brad Stone) and I actually do respect Bezos.  He's a popular villain to hate on these days, but you don't build one of the world's most valuable companies without being a genius (and making some enemies).</strong>\n",
        "</div>\n",
        "\n",
        "<div class=\"alert alert-warning\">\n",
        "  <strong>Further &mdash; a caveat with any generative model &mdash; but please don't use this notebook to make fake reviews.  It is just for education purposes.</strong>\n",
        "</div>"
      ],
      "metadata": {
        "id": "wdwYqRWuXsCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, so to fine-tune our Causal Language Model, we're going to need a dataset.  Thankfully, the Amazon Reviews dataset is here to the rescue.  There are over 50GB (uncompressed) of English-language product reviews across dozens of Amazon categories.\n",
        "\n",
        "We want to fine tune a generative model to see the prompt \"A &lt;blank&gt;-star review of the book '&lt;book title&gt;':\" and generate a review from that.  So we'll put the existing data into that format to train on."
      ],
      "metadata": {
        "id": "qDza7yNYXsCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Book Review Data"
      ],
      "metadata": {
        "id": "WOZQ4-1UXsCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-02-01T23:54:15.284186Z",
          "iopub.execute_input": "2024-02-01T23:54:15.284676Z",
          "iopub.status.idle": "2024-02-01T23:54:15.58733Z",
          "shell.execute_reply.started": "2024-02-01T23:54:15.284471Z",
          "shell.execute_reply": "2024-02-01T23:54:15.586344Z"
        },
        "trusted": true,
        "id": "M-dPDCnPXsCd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colWidth', None) # To view full text of reviews"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:54:15.588885Z",
          "iopub.execute_input": "2024-02-01T23:54:15.589404Z",
          "iopub.status.idle": "2024-02-01T23:54:15.594702Z",
          "shell.execute_reply.started": "2024-02-01T23:54:15.58937Z",
          "shell.execute_reply": "2024-02-01T23:54:15.593523Z"
        },
        "trusted": true,
        "id": "4eHzMIPFXsCd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds so notebook is reproducible\n",
        "# 設定隨機種子為 42\n",
        "np.random.seed(42)\n",
        "# 執行第一次抽樣\n",
        "print(\"--- 第一次執行，設定 seed=42 ---\")\n",
        "print(book_pdf.sample(5))\n",
        "\n",
        "# 再次設定同樣的隨機種子\n",
        "np.random.seed(42)\n",
        "# 執行第二次抽樣\n",
        "print(\"\\n--- 第二次執行，再次設定 seed=42 ---\")\n",
        "print(book_pdf.sample(5)) # 您會發現這次的結果和上面完全相同"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:54:15.597473Z",
          "iopub.execute_input": "2024-02-01T23:54:15.597823Z",
          "iopub.status.idle": "2024-02-01T23:54:15.606269Z",
          "shell.execute_reply.started": "2024-02-01T23:54:15.597794Z",
          "shell.execute_reply": "2024-02-01T23:54:15.605399Z"
        },
        "trusted": true,
        "id": "gev0P6CRXsCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eabe37b3-bee7-475b-e6ef-3e76504561d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 第一次執行，設定 seed=42 ---\n",
            "        marketplace  customer_id       review_id  product_id  product_parent  \\\n",
            "902700           US     24286685   RFO1MC91Y3FRB  1558609164       290851787   \n",
            "1769286          US     42226777   RGQC4H3CQEUNZ  0425174484       499536931   \n",
            "902908           US     24283162  R124VACIXDMLH9  0743422074       707098124   \n",
            "1972746          US     53045485  R10V9EIF70WRE7  1570622280       645081767   \n",
            "1310871          US     47485795  R3ANAJ2PHIT08N  0878332669       464577643   \n",
            "\n",
            "                                                                                                  product_title  \\\n",
            "902700   Business Intelligence: The Savvy Manager's Guide (The Morgan Kaufmann Series on Business Intelligence)   \n",
            "1769286                                      Expecting Adam: A True Story of Birth, Rebirth, and Everyday Magic   \n",
            "902908                                                                   Blink-182: Tales from Beneath Your Mom   \n",
            "1972746                                             Appreciate Your Life: Zen Teachings of Taizan Maezumi Roshi   \n",
            "1310871                                   Lost in the Mirror: An Inside Look at Borderline Personality Disorder   \n",
            "\n",
            "        product_category  star_rating  helpful_votes  total_votes vine  \\\n",
            "902700             Books          5.0           24.0         24.0    N   \n",
            "1769286            Books          5.0            2.0          3.0    N   \n",
            "902908             Books          5.0            6.0          6.0    N   \n",
            "1972746            Books          5.0           10.0         21.0    N   \n",
            "1310871            Books          5.0            6.0          7.0    N   \n",
            "\n",
            "        verified_purchase  \\\n",
            "902700                  N   \n",
            "1769286                 N   \n",
            "902908                  N   \n",
            "1972746                 N   \n",
            "1310871                 N   \n",
            "\n",
            "                                                     review_headline  \\\n",
            "902700                        Essential Reading for BI Professionals   \n",
            "1769286                                       AN IMPORTANT REMINDER!   \n",
            "902908                                                 buy this book   \n",
            "1972746                                                    excellent   \n",
            "1310871  i want to kiss him....and punch him! the truth of BPD hurts   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    review_body  \\\n",
            "902700   This book is a must read for any business or information technology (IT) professional who is involved in data warehousing and business intelligence (BI) projects. As Director of Education for The Data Warehousing Institute (TDWI) I'm intimately involved in the learning needs and processes of BI professionals. I firmly believe that the future of BI depends on better integration of technology with business at a human level. Business leaders must become more IT-savvy, and IT leaders must become more business-savvy. David Loshin's book is a fine start for both groups. You'll gain an understanding of business intelligence, business management disciplines, data warehousing, and how all of the pieces work together.<br />As a speaker at conferences and seminars I frequently challenge IT people to become more business savvy. My recommendation -- read David Loshin's &quot;Business Intelligence: The Savvy Manager's Guide&quot; first. Then read a BPM book, a CRM book, a supply chain book, and so on. If you're working in IT and have data warehouse or BI responsibilities, I make the same recommendation to you.<br />The proof, however, is in the practice. At a recent TDWI conference (San Diego, November 2003) this book sold out within the first few days of the event -- before Loshin had even arrived at the conference to teach a class. The Savvy Manager's Guide was among the top-selling books at this event and the first the be sold out.<br />Read this book. You'll be glad that you did!   \n",
            "1769286                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           I carried this book with me and read it a little at a time as I went about my daily routines - on the train, after work, during breaks in my day - I savored it in small doses. The book is a  beautiful reminder about the hidden power and energy that is constantly with us. Martha Beck writes in such a down to earth way. From the first page I felt like she was a friend writing me a letter. I would love to hear more about her life after she was Expecting Adam!!   \n",
            "902908                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       If you like the band then youll like this book. Before I bought it I was reading it at a Barnes and Noble while I was suppose to be studying and I couldnt help but laugh out loud which pissed off a few people. Reading this will make your laugh and brighten your day just like the guys in the band. This book tells how the guys got together and some of the crazy stuff they went through.   \n",
            "1972746                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  A refreshing, personal approach to zen buddhism, after the (on purpose) mind numbing of the Blue Cliff Record or the Gateless Gate.  Often very inspiring.  Right up there with Shunryu Suzuki and all the other more colorful modern Japanese lineage.  We who think we are lost, feel more found, after this finger.   \n",
            "1310871                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ...   \n",
            "\n",
            "        review_date  \n",
            "902700   2003-11-13  \n",
            "1769286  2001-11-16  \n",
            "902908   2003-11-13  \n",
            "1972746  2001-06-04  \n",
            "1310871  2002-12-10  \n",
            "\n",
            "--- 第二次執行，再次設定 seed=42 ---\n",
            "        marketplace  customer_id       review_id  product_id  product_parent  \\\n",
            "902700           US     24286685   RFO1MC91Y3FRB  1558609164       290851787   \n",
            "1769286          US     42226777   RGQC4H3CQEUNZ  0425174484       499536931   \n",
            "902908           US     24283162  R124VACIXDMLH9  0743422074       707098124   \n",
            "1972746          US     53045485  R10V9EIF70WRE7  1570622280       645081767   \n",
            "1310871          US     47485795  R3ANAJ2PHIT08N  0878332669       464577643   \n",
            "\n",
            "                                                                                                  product_title  \\\n",
            "902700   Business Intelligence: The Savvy Manager's Guide (The Morgan Kaufmann Series on Business Intelligence)   \n",
            "1769286                                      Expecting Adam: A True Story of Birth, Rebirth, and Everyday Magic   \n",
            "902908                                                                   Blink-182: Tales from Beneath Your Mom   \n",
            "1972746                                             Appreciate Your Life: Zen Teachings of Taizan Maezumi Roshi   \n",
            "1310871                                   Lost in the Mirror: An Inside Look at Borderline Personality Disorder   \n",
            "\n",
            "        product_category  star_rating  helpful_votes  total_votes vine  \\\n",
            "902700             Books          5.0           24.0         24.0    N   \n",
            "1769286            Books          5.0            2.0          3.0    N   \n",
            "902908             Books          5.0            6.0          6.0    N   \n",
            "1972746            Books          5.0           10.0         21.0    N   \n",
            "1310871            Books          5.0            6.0          7.0    N   \n",
            "\n",
            "        verified_purchase  \\\n",
            "902700                  N   \n",
            "1769286                 N   \n",
            "902908                  N   \n",
            "1972746                 N   \n",
            "1310871                 N   \n",
            "\n",
            "                                                     review_headline  \\\n",
            "902700                        Essential Reading for BI Professionals   \n",
            "1769286                                       AN IMPORTANT REMINDER!   \n",
            "902908                                                 buy this book   \n",
            "1972746                                                    excellent   \n",
            "1310871  i want to kiss him....and punch him! the truth of BPD hurts   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    review_body  \\\n",
            "902700   This book is a must read for any business or information technology (IT) professional who is involved in data warehousing and business intelligence (BI) projects. As Director of Education for The Data Warehousing Institute (TDWI) I'm intimately involved in the learning needs and processes of BI professionals. I firmly believe that the future of BI depends on better integration of technology with business at a human level. Business leaders must become more IT-savvy, and IT leaders must become more business-savvy. David Loshin's book is a fine start for both groups. You'll gain an understanding of business intelligence, business management disciplines, data warehousing, and how all of the pieces work together.<br />As a speaker at conferences and seminars I frequently challenge IT people to become more business savvy. My recommendation -- read David Loshin's &quot;Business Intelligence: The Savvy Manager's Guide&quot; first. Then read a BPM book, a CRM book, a supply chain book, and so on. If you're working in IT and have data warehouse or BI responsibilities, I make the same recommendation to you.<br />The proof, however, is in the practice. At a recent TDWI conference (San Diego, November 2003) this book sold out within the first few days of the event -- before Loshin had even arrived at the conference to teach a class. The Savvy Manager's Guide was among the top-selling books at this event and the first the be sold out.<br />Read this book. You'll be glad that you did!   \n",
            "1769286                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           I carried this book with me and read it a little at a time as I went about my daily routines - on the train, after work, during breaks in my day - I savored it in small doses. The book is a  beautiful reminder about the hidden power and energy that is constantly with us. Martha Beck writes in such a down to earth way. From the first page I felt like she was a friend writing me a letter. I would love to hear more about her life after she was Expecting Adam!!   \n",
            "902908                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       If you like the band then youll like this book. Before I bought it I was reading it at a Barnes and Noble while I was suppose to be studying and I couldnt help but laugh out loud which pissed off a few people. Reading this will make your laugh and brighten your day just like the guys in the band. This book tells how the guys got together and some of the crazy stuff they went through.   \n",
            "1972746                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  A refreshing, personal approach to zen buddhism, after the (on purpose) mind numbing of the Blue Cliff Record or the Gateless Gate.  Often very inspiring.  Right up there with Shunryu Suzuki and all the other more colorful modern Japanese lineage.  We who think we are lost, feel more found, after this finger.   \n",
            "1310871                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ...   \n",
            "\n",
            "        review_date  \n",
            "902700   2003-11-13  \n",
            "1769286  2001-11-16  \n",
            "902908   2003-11-13  \n",
            "1972746  2001-06-04  \n",
            "1310871  2002-12-10  \n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 從 100 萬筆評論中隨機抽出 5 筆\n",
        "# 因為沒有設定 seed，所以每次執行結果都不同\n",
        "print(\"--- 第一次執行 ---\")\n",
        "print(book_pdf.sample(5))\n",
        "\n",
        "print(\"\\n--- 第二次執行 ---\")\n",
        "print(book_pdf.sample(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAqRgA_xedyH",
        "outputId": "434a67f2-741a-4a9e-9e75-893acc910f72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 第一次執行 ---\n",
            "        marketplace  customer_id       review_id  product_id  product_parent  \\\n",
            "858208           US     52844621   RVXBOTCCH7V3K  1891231758       773772678   \n",
            "633267           US     42651320  R36JG1DT4GFNQT  0785261486       222922319   \n",
            "2607945          US     49942573  R3DRO7WPUBOGX6  0060192119       316982155   \n",
            "796447           US     38632362   RZBNVQ0RL7KLA  0263163687       347150430   \n",
            "2651707          US     50138045   R2YQGYNSZ5GCH  0966382064       339000526   \n",
            "\n",
            "                                                                           product_title  \\\n",
            "858208   Weight Loss Surgery: Finding the Thin Person Hiding Inside You - SECOND EDITION   \n",
            "633267                        Brainwashed: How Universities Indoctrinate America's Youth   \n",
            "2607945                             As Nature Made Him: The Boy Who Was Raised as A Girl   \n",
            "796447                                           Bartaldi's Bride (Mills & Boon Romance)   \n",
            "2651707                                                                  The Goblin King   \n",
            "\n",
            "        product_category  star_rating  helpful_votes  total_votes vine  \\\n",
            "858208             Books          5.0            4.0          5.0    N   \n",
            "633267             Books          5.0           49.0         64.0    N   \n",
            "2607945            Books          5.0           32.0         37.0    N   \n",
            "796447             Books          2.0            4.0          5.0    N   \n",
            "2651707            Books          4.0            1.0          1.0    N   \n",
            "\n",
            "        verified_purchase  \\\n",
            "858208                  N   \n",
            "633267                  Y   \n",
            "2607945                 N   \n",
            "796447                  N   \n",
            "2651707                 N   \n",
            "\n",
            "                                             review_headline  \\\n",
            "858208                                       A good resource   \n",
            "633267   Finally, a book that exposes exactly what I've seen   \n",
            "2607945                                       Simply amazing   \n",
            "796447                                          Not romantic   \n",
            "2651707                                  An Imaginary Wonder   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             review_body  \\\n",
            "858208                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           I have bought &amp; read several WLS books &amp; this by far is my favorite. I am willing to lend out the rest to my friends but this one never leaves my house. It has become my WLS Bible.<br />As a recent WLS patient, it has managed to answer almost all my questions &amp; has laid to rest many concerns. It has saved my surgeon from more than one late night telephone call.   \n",
            "633267                                                                                                                                                                                                                                                                                                                                                                                                                                                   I Just got throught reading this book, and I will tell you, this is exactly the kind of lingo I hear throughout the schools, with professors spouting their own agendas, and when you have something worth saying that completely contradicts their teachings, they just antagonize me and any other students willing to challenge their beliefs. Anybody who challenges or second guesses this book, let me say, I could've written the same book because I too have seen exactly the same things. Finally a book that speaks the truth about our professors and universities.   \n",
            "2607945  I saw part of the show where David was interviewed by Oprah and I knew I had to read the book.  The book was very well researched and thus provided the reader with enough background to understand this very delicate topic.  What makes me angry is this &quot;Dr.&quot; is still considered one of the  best.  Don't other doctors realize that this man LIED when he published the  particulars of this case? I honestly cried because of everything he put  Brenda/David through.  NOBODY deserves to have that kind of traumatic  childhood. It makes me wonder how &quot;Baby Doe&quot; has been leading her  life since she was also treated by Money.  This story should serve as an  inspiration to everyone.  I am glad that David is finally living a normal  life along with his wife and children.  It makes me happy that he is  intelligent enough to understand that his parents were doing what they  thought was best for him at the time.  This book is a MUST READ!  John  Colapinto did an excellent job!   \n",
            "796447                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I've read worse but this one made no sense. Why didn't he tell her what was going on? Why even pretend he was going to marry Paola? (BTW I love that name - Paola. Like something the mob would be handing out). And the main character, Clare,  absolutely HATED Guido the whole book and they treated each other terribly and then - Poof!- they were in love and living happily ever after. Good sexual tension but no payoff (or Paola - LOL) till the very end and no romance in the book.   \n",
            "2651707                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            When you read the Goblin King you will step into a fantasy in its truest form.It will pull you into the world of fae and conger up beautiful imaginary illustrations,that will carry you back to the innocense of  childhood. The Goblin King is well written,it moves right along,no boring  sections,and keeps renewing that spark of fantasy.I recommend reading the  Goblin King it is a book for all the dreamers of the world and those that  want to be.Kudos to Ms.Hauser, I am looking forward to reading more of her  work.   \n",
            "\n",
            "        review_date  \n",
            "858208   2003-12-21  \n",
            "633267   2004-06-11  \n",
            "2607945  2000-03-05  \n",
            "796447   2004-02-06  \n",
            "2651707  2000-02-01  \n",
            "\n",
            "--- 第二次執行 ---\n",
            "        marketplace  customer_id       review_id  product_id  product_parent  \\\n",
            "1695354          US     51210331  R21DWSDRQGNHG6  0425179494       581099109   \n",
            "42100            US     12857032  R3FNB9NERQU94D  0825611113       301314269   \n",
            "1057898          US     36232300   R30M5RZ5AD2MM  043935806X       667539744   \n",
            "2504481          US     52971741  R3G7CH26I8PV3M  1886801819       108325817   \n",
            "2048628          US     45473381  R1O8VV4KOC0WKX  0671026356       836456816   \n",
            "\n",
            "                                              product_title product_category  \\\n",
            "1695354                   The Big Nap (Mommy-Track Mystery)            Books   \n",
            "42100                         The Library of Piano Classics            Books   \n",
            "1057898  Harry Potter and the Order of the Phoenix (Book 5)            Books   \n",
            "2504481             Animating Facial Features & Expressions            Books   \n",
            "2048628     The Evil That Men Do (Buffy the Vampire Slayer)            Books   \n",
            "\n",
            "         star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
            "1695354          4.0            7.0          7.0    N                 N   \n",
            "42100            4.0           19.0         19.0    N                 N   \n",
            "1057898          3.0            1.0          1.0    N                 N   \n",
            "2504481          3.0            3.0          4.0    N                 N   \n",
            "2048628          3.0            0.0          0.0    N                 N   \n",
            "\n",
            "                                                review_headline  \\\n",
            "1695354  Funny and entertaining &quot;Mommy-Track&quot; mystery   \n",
            "42100                               Good Book of Piano Classics   \n",
            "1057898                                                   So/So   \n",
            "2504481                  Too many pictures too little tutorials   \n",
            "2048628                The Evil That Men Do lives after them...   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             review_body  \\\n",
            "1695354  The protagonist of Ayelet Waldman's &quot;The Big Nap&quot; is Juliet Applebaum, a graduate of Harvard Law and a former public defender.  After marrying the love of her life, Peter, she moves to Los Angeles.  Unfortunately, Juliet rarely sees Peter anymore, since he is busy most of the time developing a television pilot.  Juliet is now a stay-at-home mom, who dearly loves her adorable three-year-old daughter, Ruby, and her four-month-old son, Isaac.  However, Juliet is suffering from acute sleep deprivation, leaking nipples and a lack of adult conversation.<br />So what's an overtired and understimulated mother to do?  Butt into other people's business, of course!  Juliet delves into the diappearance of an eighteen-year-old Chasidic girl named Fraydle Finkelstein, who baby sat for Juliet's kids on one occasion, and who then disappeared without a trace.<br />Juliet uses her investigative powers, her contacts from her working days as a lawyer, and her innate nosiness to solve the question of what happened to Fraydle. Did the girl run away to avoid an arranged marriage she didn't want?  Or did something more sinister happen to her?  Since her parents refuse to report Freydle's disappearance to the police, Juliet feels that it is her duty to investigate.<br />When Juliet visits her mother and father in New Jersey, she even takes a side trip to Borough Park, Brooklyn.  She interviews Freydle's prospective bridegroom, and little by little, she fits the pieces together until, voila, she solves the crime.<br />Waldman has a wry and clever sense of humor, and there are many laugh-out-loud passages in &quot;The Big Nap.&quot;  In fact, the first page has such a funny scene that I laughed out loud on a public bus and drew puzzled looks from my fellow passengers.  Waldman's takes on breastfeeding, sleep-and-husband deprivation, weight gain after pregnancy and a mother's love-hate relationship with her small children are not only funny but real.<br />The mystery is not too believable, nor is it realistic that any Chasid would give Juliet the time of day, much less reveal any inside information to her.  However, the conceit of mysteries like this is that people talk to the investigator, even if she has no business asking any questions in the first place.<br />However, Waldman nicely describes some of the dynamics of the Chasidic community from the vantage point of a non-Orthodox Jew. The mystery is engrossing, if somewhat far-fetched, and you could do worse than spend an afternoon with the amusing Juliet Applebaum, mommy and sleuth.o believable, nor is it realistic that any Chasid would give Juliet the time of day, much less reveal any inside information to her.  However, the conceit of mysteries like this is that people talk to the investigator, even if she has no business asking any questions in the first place.<br />However, Waldman nicely describes some of the dynamics of the Chasidic community from the vantage point of a non-Orthodox Jew. The mystery is engrossing, if somewhat far-fetched, and you could do worse than spend an afternoon with the amusing Juliet Applebaum, mommy and sleuth.   \n",
            "42100                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 An excellent book of piano classics. Well worth having for anyone who plays from the intermediate level or higher.   \n",
            "1057898                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               I love the Harry Potter stories.  What a wonderful imagination this author has but, the Order of the Phoenix let me down a little.  I did not like the grim depressing feeling this book gave.  I understand that the book needed to take this turn somewhat as the Dark Lord is back but, I feel that there could have still been some fun times and a few good things could have happened to poor Harry along the way. It is a little complex for younger readers but gets the point across that Mrs. Rowling was going for.  Do not expect any lighthearted fun in this book.  Now that it has been proven that V..... is back I only hope that some fun times can be had even though the war to kill him will be started soon.   \n",
            "2504481                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               For Lightwave 3D and Magpie users this book covers  Human facial expressions (also a comic fish), with a lot of pictures , actually too many of them . The author could have used some space to give hands on step by  step tutorials for the actual face animation process, also not mentioned is  Morph Gizmo an essential Lightwave plugin for facial animation. Buy this  book if you like to have a complete picture library of facial poses.   \n",
            "2048628                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               the good is oft interred with their bone...  <br />This book is fairly good, but I think that the mob parts were slightly overdone.  Also, I didn't think Helen's supposed history with Angel worked.  It's not something that I see as believable.<br />What I did like a lot were the flashbacks... It was very interesting how like a slayer and how like Buffy Helen's attitude was as a gladiator.  I can see how she was seduced by Julian.<br />But one thing really bothered me the entire time... Helen and Julian are total Spike and Dru rip-offs.  And a lot less cool.  <br />(I mean really, the blond &quot;englishman&quot; completely devoted to the dark haired insane vampire that fancies herself in love with Angel.  It's all very blatant.)   \n",
            "\n",
            "        review_date  \n",
            "1695354  2002-01-13  \n",
            "42100    2005-09-21  \n",
            "1057898  2003-07-07  \n",
            "2504481  2000-04-30  \n",
            "2048628  2001-04-01  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Book Pandas DataFrame\n",
        "# We'll have more data than we can even use, so if there are any issues with a row, just skip it.\n",
        "# 檔案已經被複製並解壓縮到 Colab 的 /content 目錄下\n",
        "# 這是一個本地路徑，讀取速度非常快\n",
        "correct_local_path = '/content/amazon_reviews_us_Books_v1_02.tsv'\n",
        "\n",
        "# 使用這個正確的本地路徑\n",
        "book_pdf = pd.read_csv(correct_local_path,\n",
        "                       sep='\\t',            # .tsv 檔案需要指定分隔符是 Tab ('\\t')\n",
        "                       on_bad_lines=\"skip\")\n",
        "\n",
        "print(\"成功從 Colab 本地端讀取檔案！\")\n",
        "print(book_pdf.head())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:54:15.607695Z",
          "iopub.execute_input": "2024-02-01T23:54:15.608049Z",
          "iopub.status.idle": "2024-02-01T23:55:34.665458Z",
          "shell.execute_reply.started": "2024-02-01T23:54:15.608019Z",
          "shell.execute_reply": "2024-02-01T23:55:34.664436Z"
        },
        "trusted": true,
        "id": "Z06fvIlcXsCe",
        "outputId": "ea5df66c-2323-4177-8f89-dc4796533f72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "成功從 Colab 本地端讀取檔案！\n",
            "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
            "0          US     12076615   RQ58W7SMO911M  0385730586       122662979   \n",
            "1          US     12703090    RF6IUKMGL8SF  0811828964        56191234   \n",
            "2          US     12257412  R1DOSHH6AI622S  1844161560       253182049   \n",
            "3          US     50732546   RATOTLA3OF70O  0373836635       348672532   \n",
            "4          US     51964897  R1TNWRKIVHVYOV  0262181533       598678717   \n",
            "\n",
            "                                                    product_title  \\\n",
            "0                      Sisterhood of the Traveling Pants (Book 1)   \n",
            "1                   The Bad Girl's Guide to Getting What You Want   \n",
            "2                          Eisenhorn (A Warhammer 40,000 Omnibus)   \n",
            "3                                 Colby Conspiracy (Colby Agency)   \n",
            "4  The Psychology of Proof: Deductive Reasoning in Human Thinking   \n",
            "\n",
            "  product_category  star_rating  helpful_votes  total_votes vine  \\\n",
            "0            Books          4.0            2.0          3.0    N   \n",
            "1            Books          3.0            5.0          5.0    N   \n",
            "2            Books          4.0            1.0         22.0    N   \n",
            "3            Books          5.0            2.0          2.0    N   \n",
            "4            Books          4.0            0.0          2.0    N   \n",
            "\n",
            "  verified_purchase                        review_headline  \\\n",
            "0                 N  this book was a great learning novel!   \n",
            "1                 N                              Fun Fluff   \n",
            "2                 N                    this isn't a review   \n",
            "3                 N              fine author on her A-game   \n",
            "4                 N          Execellent cursor examination   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            review_body  \\\n",
            "0                                                                                                                                                                                                                                                                              this boook was a great one that you could learn from. it not only teaches the imponrtance of family and their values but it also deals with basic issues that teens and some kids even deal with.  this book is about 4 best friends who are for the first time in their lives spending their summer apart. one day they are all in one of the girls rooms and finds a pair of pants that were tucked away in her closet.  once all four  of them try them on they realize that there is really something special about these pants.  seeming as how all 4 girls are differnt shapes and sizes and somehow the pants fit all of them,  they realize that these pants were the start of something special.  immediatley following they decided to make up certian rules abut the pants such as you must write the best thing u did while wearing the pants over your summer on the right leg and also some silly things such as to \\\\\"never pick yuor nose while wearing the pants.\\\\\"  this book follows the girls throuh their summers in differnt places of the world and through all of the different obstacles that life takes them through. it can really teach you alot not only about what is going on around you but most imporntantly about yuorself.  i would give this book 4 stars and would reccommend it to anyone who seems the slihgtest bit interested.   \n",
            "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        If you are looking for something to stimulate your brain, this isn't it.  However, if you are just looking for a good laugh, you'll enjoy The Bad Girl's Guide.  It's funny and light, and definitely a good way to pass a little bit of time.   \n",
            "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              never read it-a young relative idicated he liked it and somehow my name popped upon this--no more to say   \n",
            "3  Though she is honored to be Chicago Woman of the Year, Victoria Colby-Camp is more euphoric over the mental improvement that her son Jim has shown recently especially since he and Tasha North fell in love.  Jim was snatched almost twenty years ago when he was seven and turned into the killing Seth whose goal was to murder Victoria for abandoning him.  However, her elation would turn to despair if she knew Seth resurfaced and started to rape a pregnant Tasha.<br /><br />Former military strategist Daniel Marks is in town complements of the Colby Agency that wants to hire him.  Also in Chicago is Emily Hastings whose father a veteran homicide detective was murdered.  She finds letters linking her dad to Victoria, the woman's long ago murdered first husband James, and her dad's first partner Marelyn Rutland that confuses her.  Soon she will meet Daniel and they will be embroiled in the COLBY CONSPIRACY that goes back almost two decades ago.<br /><br />Though the subplots can become confusing at first, once the audience comprehends how this complex superb suspense thriller starts to come together, they will want more Colby Agency tales; (see FILES FROM THE COLBY AGENCY: THE BODYGUARD'S BABY PROTECTIVE CUSTODY).  The ensemble cast is solid as fans will feel with Victoria who has overcome so much tragedy, hope Jim \\\\\"defeats\\\\\" Seth with Tasha at his side, and root for Daniel and Emily to make it while wondering what really happened two decades ago.  A final twist marks a strong Webb of deceit tale that showcases a fine author on her A-game.<br /><br />Harriet Klausner   \n",
            "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Review based on a cursory examination by University of Phoenix students in Philosophy 251.  <br />We have found the book to be well organized, and detailed. The structure placed the information in an easy to read and presentable manner. <br />The use of strong and sound proofs by experiment and examples supported the overall logic of reasoning in a clear and concise manner. <br />We have found the book to be well thought out and laid out so the reader can learn and follow what is being explained. <br />   \n",
            "\n",
            "  review_date  \n",
            "0  2005-10-14  \n",
            "1  2005-10-14  \n",
            "2  2005-10-14  \n",
            "3  2005-10-14  \n",
            "4  2005-10-14  \n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the shape of the data and the column headers we have\n",
        "book_pdf.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:55:34.666496Z",
          "iopub.execute_input": "2024-02-01T23:55:34.667351Z",
          "iopub.status.idle": "2024-02-01T23:55:34.703798Z",
          "shell.execute_reply.started": "2024-02-01T23:55:34.667298Z",
          "shell.execute_reply": "2024-02-01T23:55:34.702445Z"
        },
        "trusted": true,
        "id": "YWG-EfBoXsCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62c9aa61-e8d1-41dc-f490-e42eb74c1792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3105370 entries, 0 to 3105369\n",
            "Data columns (total 15 columns):\n",
            " #   Column             Dtype  \n",
            "---  ------             -----  \n",
            " 0   marketplace        object \n",
            " 1   customer_id        int64  \n",
            " 2   review_id          object \n",
            " 3   product_id         object \n",
            " 4   product_parent     int64  \n",
            " 5   product_title      object \n",
            " 6   product_category   object \n",
            " 7   star_rating        float64\n",
            " 8   helpful_votes      float64\n",
            " 9   total_votes        float64\n",
            " 10  vine               object \n",
            " 11  verified_purchase  object \n",
            " 12  review_headline    object \n",
            " 13  review_body        object \n",
            " 14  review_date        object \n",
            "dtypes: float64(3), int64(2), object(10)\n",
            "memory usage: 355.4+ MB\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from this summary, there are about 3.1M book review entries loaded.  In the end, we'll only use the `product_title`, `star_rating`, and `review_headline`.  \n",
        "\n",
        "The `review_headline` is the very brief review summary that you can write when leaving an Amazon review.  We'll use this instead of the full-length `review_body` for two reasons.\n",
        "\n",
        "1. To save on required memory and runtime.\n",
        "2. It's already a hard task to write a fake book review from only the title and the star rating.  Think about doing this yourself.. if you know nothing about the book but the title, how are you going to write a very detailed review?  Thus we want these reviews to be shorter and therefore somewhat more generic."
      ],
      "metadata": {
        "id": "cfQVif5-XsCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze & Clean the Data"
      ],
      "metadata": {
        "id": "cl1MWR-ZXsCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop any rows that don't have star ratings, product_titles, or review_headlines\n",
        "book_pdf = book_pdf.dropna(subset=[\"star_rating\", \"product_title\", \"review_headline\"])\n",
        "print(\"Datapoint entries: \", book_pdf.shape[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:55:34.707231Z",
          "iopub.execute_input": "2024-02-01T23:55:34.707556Z",
          "iopub.status.idle": "2024-02-01T23:55:35.807778Z",
          "shell.execute_reply.started": "2024-02-01T23:55:34.707528Z",
          "shell.execute_reply": "2024-02-01T23:55:35.807011Z"
        },
        "trusted": true,
        "id": "zLs2YRZ1XsCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "042824ce-466c-4d20-9ea3-57eabd22f70d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datapoint entries:  1902896\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few were dropped because they had blank `star_rating` or `review_headline`s.\n",
        "\n",
        "Next, we'll drop any reviews that are not verified, so we can be more certain that the \"real\" reviews we're training on, are indeed actually real."
      ],
      "metadata": {
        "id": "RGNB1GvbXsCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get rid of any non-verified reviews,\n",
        "# so we can be more certain that the reviews we think are real, are real.\n",
        "\n",
        "# first reset the index when using .index attribute, ensures index is unique\n",
        "print(book_pdf.columns)\n",
        "# book_pdf = book_pdf.reset_index(drop=True)\n",
        "# book_pdf = book_pdf.drop(book_pdf[book_pdf.verified_purchase != 'Y'].index)\n",
        "# print(\"Datapoint entries: \", book_pdf.shape[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:55:35.808941Z",
          "iopub.execute_input": "2024-02-01T23:55:35.809945Z",
          "iopub.status.idle": "2024-02-01T23:55:37.783433Z",
          "shell.execute_reply.started": "2024-02-01T23:55:35.809905Z",
          "shell.execute_reply": "2024-02-01T23:55:37.7826Z"
        },
        "trusted": true,
        "id": "HeNhP_mwXsCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d62621e8-3da0-4f66-f67d-b13c72cacc4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['product_title', 'star_rating', 'review_headline'], dtype='object')\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This *greatly* reduced the size of the dataset, to about a tenth of its original size, at 229K entries.  That's OK for this project, as we will drop more later to make the reviews balanced, and it's still more than we need for fine-tuning.  Because a Large Language Model comes with so much inherent \"knowledge\" about the world pre-existing in its parameters, it needs much less data to fine-tune it for a task than if you were to start from scratch.  A smaller dataset is also less unwieldy, and we won't need to use generator functions, as we can just store it all in GPU and system RAM.\n",
        "\n",
        "Let's further reduce the size (and improve the quality of the training data) by removing reviews that don't have a least 3 \"helpful\" upvotes on it.  This will serve as a proxy for them being likely genuine, as reviews that are low-quality, self-promotional, or clearly written by a competing Amazon seller will probably be noticed by humans and not marked as helpful."
      ],
      "metadata": {
        "id": "O1cxnydoXsCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll also look for something to have at least 3 \"helpful\" upvotes on\n",
        "# the reviews as a proxy for them being likely genuine.  Stuff that's too\n",
        "# spammy/markety probably won't get marked as helpful.\n",
        "\n",
        "# first reset the index when using .index attribute, ensures index is unique\n",
        "book_pdf.reset_index()\n",
        "book_pdf = book_pdf.drop(book_pdf[book_pdf.helpful_votes < 3].index)\n",
        "print(\"Datapoint entries: \", book_pdf.shape[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:55:37.803569Z",
          "iopub.execute_input": "2024-02-01T23:55:37.804311Z",
          "iopub.status.idle": "2024-02-01T23:55:38.409502Z",
          "shell.execute_reply.started": "2024-02-01T23:55:37.804275Z",
          "shell.execute_reply": "2024-02-01T23:55:38.40822Z"
        },
        "trusted": true,
        "id": "1-enk5HuXsCf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f05279fe-051a-4043-911f-b98097e2b773"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datapoint entries:  1902922\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've reduced it by about half again, leaving only reviews that are helpful (and thus higher quality).\n",
        "\n",
        "We're now finished using any of the extra columns that we won't direcly use for training, so we can drop them."
      ],
      "metadata": {
        "id": "2sYSIXDvXsCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only a few columns we care about:\n",
        "# product_title, star_rating, and review_headline\n",
        "\n",
        "# Drop all columns except those to speed up future processing\n",
        "book_pdf = book_pdf[['product_title', 'star_rating', 'review_headline']]\n",
        "book_pdf.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:55:38.699435Z",
          "iopub.execute_input": "2024-02-01T23:55:38.699819Z",
          "iopub.status.idle": "2024-02-01T23:55:38.86109Z",
          "shell.execute_reply.started": "2024-02-01T23:55:38.699792Z",
          "shell.execute_reply": "2024-02-01T23:55:38.85973Z"
        },
        "trusted": true,
        "id": "BW7jNFbpXsCf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24464ba3-cea9-45ad-b73f-7f2e18983f07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1902922 entries, 1 to 3105369\n",
            "Data columns (total 3 columns):\n",
            " #   Column           Dtype  \n",
            "---  ------           -----  \n",
            " 0   product_title    object \n",
            " 1   star_rating      float64\n",
            " 2   review_headline  object \n",
            "dtypes: float64(1), object(2)\n",
            "memory usage: 58.1+ MB\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# book_pdf.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:55:38.86308Z",
          "iopub.execute_input": "2024-02-01T23:55:38.863411Z",
          "iopub.status.idle": "2024-02-01T23:55:38.869041Z",
          "shell.execute_reply.started": "2024-02-01T23:55:38.863383Z",
          "shell.execute_reply": "2024-02-01T23:55:38.867586Z"
        },
        "trusted": true,
        "id": "zWkxOn0_XsCf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's clean up some funky stuff in the text so it doesn't get output when we generate later.  We don't want to do too much preprocessing here, as tokenizer should be able to handle these.  Let's just get rid of reviews that contain URLs or HTML tags."
      ],
      "metadata": {
        "id": "MdrWMHEJXsCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's just delete any reviews that contain html tags or urls.\n",
        "# My hunch is these aren't common in a legit review, and urls\n",
        "# seem like they'd be abused by marketers linking out, so this\n",
        "# can be another cheap form of pruning anything but real reviews.\n",
        "\n",
        "# Re-index since we've dropped many.  Probably not necessary.\n",
        "book_pdf.reset_index()\n",
        "\n",
        "# Amazon has already converted the opening tags of HTML into &lt;, but not the closing tag,\n",
        "# e.g. &lt;a href=\"blah\">, so we can just look for this simple pattern and drop rows that\n",
        "# contain it.  This will automatically cover urls too.\n",
        "html_delete = \"&lt;.*?>\"\n",
        "# Can use the .str.contains() method\n",
        "book_pdf = book_pdf.drop(book_pdf[book_pdf.review_headline.str.contains(html_delete)].index)\n",
        "print(\"Datapoint entries: \", book_pdf.shape[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:55:38.870386Z",
          "iopub.execute_input": "2024-02-01T23:55:38.870717Z",
          "iopub.status.idle": "2024-02-01T23:55:39.014089Z",
          "shell.execute_reply.started": "2024-02-01T23:55:38.870685Z",
          "shell.execute_reply": "2024-02-01T23:55:39.012838Z"
        },
        "trusted": true,
        "id": "4C7neZUvXsCf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "0e248810-7c54-4afe-a38a-d54c26b4238e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot mask with non-boolean array containing NA / NaN values",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1165231957.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mhtml_delete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"&lt;.*?>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Can use the .str.contains() method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mbook_pdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbook_pdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook_pdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbook_pdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview_headline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml_delete\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Datapoint entries: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbook_pdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4091\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4092\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4093\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mis_bool_indexer\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0;31m# Don't raise on e.g. [\"A\", \"B\", np.nan], see\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0;31m#  test_loc_getitem_list_of_labels_categoricalindex_with_na\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mna_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot mask with non-boolean array containing NA / NaN values"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a brief look again at the data\n",
        "book_pdf.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:55:39.0155Z",
          "iopub.execute_input": "2024-02-01T23:55:39.015801Z",
          "iopub.status.idle": "2024-02-01T23:55:39.038103Z",
          "shell.execute_reply.started": "2024-02-01T23:55:39.015775Z",
          "shell.execute_reply": "2024-02-01T23:55:39.036772Z"
        },
        "trusted": true,
        "id": "u1i0j-f4XsCf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now take a look at the distribution of star ratings to see how balanced our dataset is."
      ],
      "metadata": {
        "id": "nTmQmfzvXsCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries for plotting\n",
        "import seaborn as sns            # For visualizations\n",
        "import matplotlib.pyplot as plt  # For plotting"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:55:39.03964Z",
          "iopub.execute_input": "2024-02-01T23:55:39.039975Z",
          "iopub.status.idle": "2024-02-01T23:55:39.623169Z",
          "shell.execute_reply.started": "2024-02-01T23:55:39.039946Z",
          "shell.execute_reply": "2024-02-01T23:55:39.621998Z"
        },
        "trusted": true,
        "id": "z_ZBrpHiXsCf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# General function to generate column chart for any given attribute\n",
        "def column_chart(attribute, label):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.countplot(data=book_pdf, x=attribute)\n",
        "    plt.xlabel(label)\n",
        "    plt.ylabel('Count')\n",
        "    plt.title(f'Distribution of {label}')\n",
        "    plt.show()\n",
        "\n",
        "column_chart('star_rating', 'Star Ratings')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:55:39.624795Z",
          "iopub.execute_input": "2024-02-01T23:55:39.62784Z",
          "iopub.status.idle": "2024-02-01T23:55:39.855769Z",
          "shell.execute_reply.started": "2024-02-01T23:55:39.627797Z",
          "shell.execute_reply": "2024-02-01T23:55:39.854387Z"
        },
        "trusted": true,
        "id": "t5lKUhUEXsCf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "book_pdf['star_rating'].value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:55:39.857169Z",
          "iopub.execute_input": "2024-02-01T23:55:39.857537Z",
          "iopub.status.idle": "2024-02-01T23:55:39.869734Z",
          "shell.execute_reply.started": "2024-02-01T23:55:39.857505Z",
          "shell.execute_reply": "2024-02-01T23:55:39.868236Z"
        },
        "trusted": true,
        "id": "qWTP6N7WXsCg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are far more 5 star reviews than anything else, by a factor of about 7x.\n",
        "\n",
        "The easiest way to balance these is to just chop them all down to the minimal bucket, which would be 2-star reviews.  That will still be enough to train on, and again will help with memory usage.\n",
        "\n",
        "But before we chop off the dataset, I actually want to leave just the shortest reviews to train on, because the longer a generated review is, the more obvious it will be that it's fake.  That's because there isn't enough context just from the title of a book to really say much specific about the book.\n",
        "\n",
        "So let's sort our dataset on the `review_headline` column, so that when we chop off the datapoints, we leave the shortest ones.  We'll specify a minimum length of 15 characters for the review headline, otherwise we get reviews that are like \"!\" or \"1\"."
      ],
      "metadata": {
        "id": "uDYfMygYXsCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the dataset by the length of the review_headline entries\n",
        "\n",
        "# Add a new column with the length of the 'review_headline'\n",
        "book_pdf['headline_length'] = book_pdf['review_headline'].apply(len)\n",
        "\n",
        "# Sort the DataFrame based on the 'headline_length' column\n",
        "book_pdf = book_pdf.sort_values(by='headline_length')\n",
        "\n",
        "# Drop the additional column since we don't need it in the final result\n",
        "book_pdf = book_pdf.drop(columns=['headline_length'])\n",
        "\n",
        "# But also drop reviews that are less than a certain length\n",
        "book_pdf = book_pdf[book_pdf['review_headline'].apply(len) >= 15]\n",
        "\n",
        "# Display the head of the sorted data\n",
        "print(book_pdf.head())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:55:39.870933Z",
          "iopub.execute_input": "2024-02-01T23:55:39.871263Z",
          "iopub.status.idle": "2024-02-01T23:55:40.050417Z",
          "shell.execute_reply.started": "2024-02-01T23:55:39.871232Z",
          "shell.execute_reply": "2024-02-01T23:55:40.049503Z"
        },
        "trusted": true,
        "id": "pyQ89n2FXsCg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "book_pdf['star_rating'].value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:55:40.051765Z",
          "iopub.execute_input": "2024-02-01T23:55:40.052069Z",
          "iopub.status.idle": "2024-02-01T23:55:40.063195Z",
          "shell.execute_reply.started": "2024-02-01T23:55:40.052045Z",
          "shell.execute_reply": "2024-02-01T23:55:40.062376Z"
        },
        "trusted": true,
        "id": "GbsxnTfRXsCg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we're ready to chop the data down to a balanced number per bucket.  We'll shuffle the data first, then use the smallest number for pruning, which is 9450 reviews in the 2-star category."
      ],
      "metadata": {
        "id": "8CvWj6SOXsCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle all the rows first, fraction of 1 (100%)\n",
        "book_pdf = book_pdf.sample(frac=1)\n",
        "\n",
        "# How many examples we want from each star-rating class\n",
        "num_samples = 9450\n",
        "\n",
        "# Drop everything after the first K samples, for each star rating\n",
        "book_pdf.drop(book_pdf[book_pdf.star_rating == 5.0].index[num_samples:], inplace=True)\n",
        "book_pdf.drop(book_pdf[book_pdf.star_rating == 4.0].index[num_samples:], inplace=True)\n",
        "book_pdf.drop(book_pdf[book_pdf.star_rating == 3.0].index[num_samples:], inplace=True)\n",
        "book_pdf.drop(book_pdf[book_pdf.star_rating == 2.0].index[num_samples:], inplace=True)\n",
        "book_pdf.drop(book_pdf[book_pdf.star_rating == 1.0].index[num_samples:], inplace=True)\n",
        "\n",
        "book_pdf['star_rating'].value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:55:40.064128Z",
          "iopub.execute_input": "2024-02-01T23:55:40.064441Z",
          "iopub.status.idle": "2024-02-01T23:55:40.174737Z",
          "shell.execute_reply.started": "2024-02-01T23:55:40.064413Z",
          "shell.execute_reply": "2024-02-01T23:55:40.174077Z"
        },
        "trusted": true,
        "id": "ZQkjNqORXsCg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "column_chart('star_rating', 'Star Ratings')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:55:40.176263Z",
          "iopub.execute_input": "2024-02-01T23:55:40.176523Z",
          "iopub.status.idle": "2024-02-01T23:55:40.374107Z",
          "shell.execute_reply.started": "2024-02-01T23:55:40.176499Z",
          "shell.execute_reply": "2024-02-01T23:55:40.372631Z"
        },
        "trusted": true,
        "id": "bvN5W964XsCg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Refactoring the Text Sequences for Fine-Tuning\n",
        "\n",
        "We want to fine-tune in such a way that we can generate the fake reviews later.  To generate a fake review, we'll need the title and the star rating we desire for the review.  The generative transformer model should then generate a fake review based solely on that.  So we need to get our dataset into a single column of sentences of that same structure to use as training examples.\n",
        "\n",
        "For example:\n",
        "'A 3-star review of the book \"The Bad Girl's Guide to Getting What You Want\": '\n",
        "\n",
        "Note that we also could just do something like \"&lt;star&gt; &lt;title&gt;:\" but that wouldn't take any advantage of the inherent knowledge that the LLM has about the world.  We are giving it some more context that we're desiring a book review that corresponds to a particular star rating.\n",
        "\n",
        "Let's get our dataset into that format now."
      ],
      "metadata": {
        "id": "bHt7XLjgXsCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a new column containing the concatenation of the other columns,\n",
        "# with our separator as described.\n",
        "# Need to escape the \"\" that are meant to be left in the sequence.\n",
        "# Also turn the star rating from a float to an int.\n",
        "# e.g. \"A 5-star review of the book \\\"The Evolution of Useful Things\\\": \"\n",
        "book_pdf['concat_sequence'] = book_pdf.apply(lambda x:\n",
        "                                             'A ' + str(int(x['star_rating']))\n",
        "                                             + '-star review of the book \\\"'\n",
        "                                             + x['product_title'] + '\\\": '\n",
        "                                             + x['review_headline'],\n",
        "                                             axis=1)\n",
        "\n",
        "book_pdf.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:59:21.049708Z",
          "iopub.execute_input": "2024-02-01T23:59:21.051785Z",
          "iopub.status.idle": "2024-02-01T23:59:21.731379Z",
          "shell.execute_reply.started": "2024-02-01T23:59:21.051729Z",
          "shell.execute_reply": "2024-02-01T23:59:21.729892Z"
        },
        "trusted": true,
        "id": "G3ixBrjTXsCg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at a histogram of the final concatenated sequence lengths (the training phrases)."
      ],
      "metadata": {
        "id": "9JkRPePYXsCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the lengths of strings in the concatenated sequences\n",
        "book_pdf['String_Length'] = book_pdf['concat_sequence'].apply(len)\n",
        "\n",
        "# Plot histogram\n",
        "plt.hist(book_pdf['String_Length'], bins=range(min(book_pdf['String_Length']),\n",
        "                                               max(book_pdf['String_Length']) + 1),\n",
        "                                    edgecolor='black')\n",
        "plt.title('String Length Histogram (Characters)')\n",
        "plt.xlabel('String Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:55:41.000727Z",
          "iopub.execute_input": "2024-02-01T23:55:41.001486Z",
          "iopub.status.idle": "2024-02-01T23:55:41.974338Z",
          "shell.execute_reply.started": "2024-02-01T23:55:41.001459Z",
          "shell.execute_reply": "2024-02-01T23:55:41.972928Z"
        },
        "trusted": true,
        "id": "QCGiXz96XsCg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "And as a word count.."
      ],
      "metadata": {
        "id": "xS8f-vtaXsCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of words in each string in concatenated sequences\n",
        "book_pdf['Word_Count'] = book_pdf['concat_sequence'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Plot histogram\n",
        "plt.hist(book_pdf['Word_Count'], bins=range(min(book_pdf['Word_Count']),\n",
        "                                            max(book_pdf['Word_Count']) + 1),\n",
        "                                 edgecolor='black')\n",
        "plt.title('Word Count Histogram')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T23:55:41.975775Z",
          "iopub.execute_input": "2024-02-01T23:55:41.976121Z",
          "iopub.status.idle": "2024-02-01T23:55:42.396105Z",
          "shell.execute_reply.started": "2024-02-01T23:55:41.976093Z",
          "shell.execute_reply": "2024-02-01T23:55:42.395071Z"
        },
        "trusted": true,
        "id": "ZU88b7kqXsCk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that most reviews have 40 words or less, but that there is a long tail of longer review headlines.  Note that this is the entire concatenated sequence, so some of these could also be very long book titles as well.  This also includes our prompt prepended hint \"A X-star review of the book:\".  As a sanity check, we can see from the histogram that the shortest this could possibly be (if we had a single word book title) is 7 words, which seems right from the histogram."
      ],
      "metadata": {
        "id": "ZT35PD74XsCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start of Transformer model"
      ],
      "metadata": {
        "id": "0WJa2ZiEXsCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the DistilGPT2 transformer model from Hugging Face for this project.\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "  <strong>\n",
        "      📝 DistilGPT2 is a smaller version of GPT2 (from the original OpenAI paper) that has 82M parameters instead of the original 124M, reduced using knowledge distillation.  Ends up being 2x faster than GPT2 and nearly the same performance (in terms of predicting the next word accuracy).  For this simple task, I don't need a too-powerful model and this should suffice.\n",
        "    </strong>\n",
        "</div>"
      ],
      "metadata": {
        "id": "KBqEwwUzXsCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForCausalLM"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:37:55.66041Z",
          "iopub.execute_input": "2024-02-01T20:37:55.660699Z",
          "iopub.status.idle": "2024-02-01T20:38:09.829824Z",
          "shell.execute_reply.started": "2024-02-01T20:37:55.660659Z",
          "shell.execute_reply": "2024-02-01T20:38:09.829003Z"
        },
        "trusted": true,
        "id": "DQi_8iFYXsCl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow seed for reproducibility\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:09.831071Z",
          "iopub.execute_input": "2024-02-01T20:38:09.831794Z",
          "iopub.status.idle": "2024-02-01T20:38:09.83656Z",
          "shell.execute_reply.started": "2024-02-01T20:38:09.831758Z",
          "shell.execute_reply": "2024-02-01T20:38:09.835679Z"
        },
        "trusted": true,
        "id": "Z0GD6YkzXsCl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Use DistilGPT2, a more efficient version of GPT-2\n",
        "checkpoint = \"distilgpt2\"\n",
        "\n",
        "# Added this padding_side argument so that later when I pad batches,\n",
        "# it doesn't pad the right side (since decoder-only architecture!)\n",
        "# Without this argument, we'd later have issues during inference if\n",
        "# we want to do a batch of prompts all at once.  Since they are different\n",
        "# lengths, some would need to be padded, so we need to define that when\n",
        "# we initialize the tokenizer.\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, padding_side='left')\n",
        "\n",
        "# Reproducible results\n",
        "tokenizer.seed = 42\n",
        "\n",
        "# Use the Hugging Face model architecture for causal language models (CLM),\n",
        "# TensorFlow variety.  Also, has 81M parameters, which seems high for\n",
        "# fine-tuning (although maybe you fine-tune all of them).\n",
        "model = TFAutoModelForCausalLM.from_pretrained(checkpoint)\n",
        "\n",
        "# Seed all random generators\n",
        "model.config.seed = 42"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:09.838109Z",
          "iopub.execute_input": "2024-02-01T20:38:09.838742Z",
          "iopub.status.idle": "2024-02-01T20:38:26.049176Z",
          "shell.execute_reply.started": "2024-02-01T20:38:09.838706Z",
          "shell.execute_reply": "2024-02-01T20:38:26.048466Z"
        },
        "trusted": true,
        "id": "V4Zoqv4aXsCl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Just a double check of the tokenized data for an example sentence\n",
        "\n",
        "text = \"Oh HAI, I'm just a plan 'ol input sentence prompt.\"\n",
        "\n",
        "encoded_input = tokenizer(text, return_tensors='tf')\n",
        "print(encoded_input)\n",
        "\n",
        "output = model(encoded_input)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:26.05029Z",
          "iopub.execute_input": "2024-02-01T20:38:26.050576Z",
          "iopub.status.idle": "2024-02-01T20:38:26.164579Z",
          "shell.execute_reply.started": "2024-02-01T20:38:26.050551Z",
          "shell.execute_reply": "2024-02-01T20:38:26.163723Z"
        },
        "trusted": true,
        "id": "EfXa5aGyXsCl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "On this particular example, this uses 15 tokens.  Subtract two for the start and end tokens, and we're at about 13 tokens for 10 words and 4 punctuation marks.  This isn't a perfect mapping, but maybe it's around a token per word for this tokenizer, on average.  Just trying to get ballpark here."
      ],
      "metadata": {
        "id": "zpsGG7P3XsCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Pipeline Results\n",
        "\n",
        "Let's see how this bad boy does out of the box on an example.  We've given it **no** clues what it should do with this prompt."
      ],
      "metadata": {
        "id": "POQWtyWSXsCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "\n",
        "generator = pipeline('text-generation', model=checkpoint)\n",
        "\n",
        "# Make the pipeline's randomness deterministic too\n",
        "set_seed(42)\n",
        "\n",
        "generator(\"A 5-star review of the book \\\"Three Steps to Yes: The Gentle Art of Getting Your Way\\\": \",\n",
        "          max_length=64,\n",
        "          num_return_sequences=1,\n",
        "          pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:26.165926Z",
          "iopub.execute_input": "2024-02-01T20:38:26.16628Z",
          "iopub.status.idle": "2024-02-01T20:38:30.53386Z",
          "shell.execute_reply.started": "2024-02-01T20:38:26.166246Z",
          "shell.execute_reply": "2024-02-01T20:38:30.532956Z"
        },
        "trusted": true,
        "id": "m3XDgNjqXsCl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "generator(\"A 1-star review of the book \\\"The Evolution of Useful Things\\\": \",\n",
        "          max_length=64,\n",
        "          num_return_sequences=1,\n",
        "          pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:30.539823Z",
          "iopub.execute_input": "2024-02-01T20:38:30.540113Z",
          "iopub.status.idle": "2024-02-01T20:38:31.558935Z",
          "shell.execute_reply.started": "2024-02-01T20:38:30.540089Z",
          "shell.execute_reply": "2024-02-01T20:38:31.558013Z"
        },
        "trusted": true,
        "id": "nFWLY_vDXsCl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not great!\n",
        "\n",
        "At other various times running this pipeline (out-of-the-box) model, many times the output was blank.  Once it gave me a charming anecdote about the reviewer's time in their college dorms.  But alas, nothing resembling book reviews.\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "  <strong>\n",
        "    📝 Of course, the model doesn't know what we want to do, but I would have thought with that very minimal information that it could do a decent job based on its built-in knowledge.  But GPT2 is from 2019, and is a much smaller model (let alone the distilled version as in here) with worse performance than we're used to these days with ChatGPT and such (1.5B parameters vs. 82M here).  \n",
        "    </strong>\n",
        "</div>\n",
        "\n",
        "Let's now fine-tune it and see if we can do better."
      ],
      "metadata": {
        "id": "IT1hXDPnXsCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the data for fine-tuning\n",
        "\n",
        "We'll need to truncate and pad the dataset.  \n",
        "\n",
        "Looking at the histogram, we can see that most of the reviews fit into the 40-word range.  As we saw earlier, it's also not straightforward to determine how many tokens it takes to make the average word, but if we guess that it's a little more than one token per word (using more tokens for less frequent word pieces), we can round this up to a nice power of 2 and pick a `max_length` for a sequence of 64 tokens.  A power of 2 is nice because it helps fit into a matrix and memory more efficiently, though it's not crucial."
      ],
      "metadata": {
        "id": "E56uvAgBXsCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer key parameters\n",
        "max_length = 64\n",
        "batch_size = 32"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:31.560168Z",
          "iopub.execute_input": "2024-02-01T20:38:31.560453Z",
          "iopub.status.idle": "2024-02-01T20:38:31.564875Z",
          "shell.execute_reply.started": "2024-02-01T20:38:31.560429Z",
          "shell.execute_reply": "2024-02-01T20:38:31.563843Z"
        },
        "trusted": true,
        "id": "pMDtJtWvXsCm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch encode the data"
      ],
      "metadata": {
        "id": "KPmSMRXGXsCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the padding token to the EOS token.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# NOTE:\n",
        "# Couldn't get this to work without using .tolist() method, but this isn't ideal\n",
        "# because it means the entire dataset needs to fit into RAM!  Had to shrink my\n",
        "# training data to make this work.\n",
        "\n",
        "# Next time, look more into 'prepare_tf_dataset()' and 'to_tf_dataset()' methods.\n",
        "# Tried the 'to_tf_dataset()' method already, but doesn't work with the batch-encoded\n",
        "# methods.  Possibly instead use 'prepare_tf_dataset()' as shown here:\n",
        "# https://huggingface.co/docs/transformers/training\n",
        "# because it looks like this might batch as it goes, fixing the RAM issues.\n",
        "\n",
        "# Tokenize the entire column using batch_encode_plus\n",
        "tokenized_data = tokenizer.batch_encode_plus(\n",
        "    book_pdf['concat_sequence'].tolist(),\n",
        "    return_tensors='tf',\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=max_length\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:31.565987Z",
          "iopub.execute_input": "2024-02-01T20:38:31.566275Z",
          "iopub.status.idle": "2024-02-01T20:38:35.046655Z",
          "shell.execute_reply.started": "2024-02-01T20:38:31.566253Z",
          "shell.execute_reply": "2024-02-01T20:38:35.045824Z"
        },
        "trusted": true,
        "id": "6kbJC_0hXsCm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the tokenized data\n",
        "print(\"Input IDs:\", tokenized_data['input_ids'])\n",
        "print(\"Attention Mask:\", tokenized_data['attention_mask'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:35.047825Z",
          "iopub.execute_input": "2024-02-01T20:38:35.048173Z",
          "iopub.status.idle": "2024-02-01T20:38:35.05481Z",
          "shell.execute_reply.started": "2024-02-01T20:38:35.048143Z",
          "shell.execute_reply": "2024-02-01T20:38:35.053721Z"
        },
        "trusted": true,
        "id": "BSjrLj9GXsCm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Examine some of the data types I'm dealing with\n",
        "# print(type(tokenized_data))\n",
        "# print(type(tokenized_data['input_ids']))\n",
        "# print(tokenized_data['input_ids'][:1])\n",
        "# print(tokenized_data.keys())\n",
        "\n",
        "# The tokenized data only has keys 'input_ids' and 'attention_mask' for\n",
        "# CLM (Causal Language Modeling)\n",
        "\n",
        "# Prints:\n",
        "# <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
        "# <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
        "# tf.Tensor(\n",
        "# [[50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
        "#   50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
        "#   50256 50256 50256 50256 50256 50256 50256 50256 50256    32   642    12\n",
        "#    7364  2423   286   262  1492   366  5756  4021 12167   286  4650 11597\n",
        "#    1058   383  9745   276  2531 16672   351   968 45465   416   262  6434\n",
        "#    1298  1052  1605 13745]], shape=(1, 64), dtype=int32)\n",
        "# dict_keys(['input_ids', 'attention_mask'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:35.056238Z",
          "iopub.execute_input": "2024-02-01T20:38:35.056957Z",
          "iopub.status.idle": "2024-02-01T20:38:36.322924Z",
          "shell.execute_reply.started": "2024-02-01T20:38:35.056921Z",
          "shell.execute_reply": "2024-02-01T20:38:36.321994Z"
        },
        "trusted": true,
        "id": "LkGMWC2dXsCn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to Tensorflow Dataset\n",
        "\n",
        "Prepare the data for a Causal Language Model (where our task is to predict the next word).\n",
        "\n",
        "In a CLM dataset, the label is just the next word!  Note that the model already handles the shifting where the next word becomes the label for the current word, so we don't want to do this here or words would get doubly shifted.  In other words, for this particular model, the label for each word is just itself, which you can see in the code below."
      ],
      "metadata": {
        "id": "ZV3KiNAjXsCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to build our own causal dataset, where the labels are the same as the input tokens.\n",
        "# Couldn't use the datacollatorforlanguagemodeling because the .to_tf_dataset() method\n",
        "# is for HF dataset type, not BatchEncoding type.\n",
        "\n",
        "# Convert to TensorFlow Dataset\n",
        "tfds = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': tokenized_data['input_ids'],\n",
        "        'attention_mask': tokenized_data['attention_mask']\n",
        "    },\n",
        "    tokenized_data['input_ids']  # this becomes the labels, labels are just the next word\n",
        "                                 # (shifted internally inside the model)\n",
        "))\n",
        "\n",
        "# Batch the dataset (already shuffled earlier)\n",
        "tfds = tfds.batch(batch_size=batch_size)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:36.324171Z",
          "iopub.execute_input": "2024-02-01T20:38:36.324453Z",
          "iopub.status.idle": "2024-02-01T20:38:36.345434Z",
          "shell.execute_reply.started": "2024-02-01T20:38:36.32443Z",
          "shell.execute_reply": "2024-02-01T20:38:36.344708Z"
        },
        "trusted": true,
        "id": "31A30dx_XsCn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's examine the first batched example from the dataset\n",
        "for input_batch, label_batch in tfds.take(1):\n",
        "    print(\"Input IDs:\", input_batch['input_ids'])\n",
        "    print(\"Attention Mask:\", input_batch['attention_mask'])\n",
        "    print(\"Label:\", label_batch)\n",
        "    print(\"=\" * 50)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:36.346426Z",
          "iopub.execute_input": "2024-02-01T20:38:36.346727Z",
          "iopub.status.idle": "2024-02-01T20:38:36.395711Z",
          "shell.execute_reply.started": "2024-02-01T20:38:36.346676Z",
          "shell.execute_reply": "2024-02-01T20:38:36.394801Z"
        },
        "trusted": true,
        "id": "yBwuserDXsCn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, we printed out the tokenized data for a single batch.  Let's break it down and see if it seems correct.\n",
        "\n",
        "For a single batch, there should be three dictionary keys for the CLM task: input token IDs, the attention mask (what the transformer should attend to), and the label for which word to predict next.  We do see those three keys in the printout above.  We've also set it for a batch size of 32, and with a max sequence length of 64.  We can see that each key has the correct shape.  We can see that every input ID token is the same as the label token, which as we discussed above, is correct, since the model will shift these internally by one for us.\n",
        "\n",
        "<div class=\"alert alert-warning\">\n",
        "  <strong>Now here's something interesting</strong>.. why is every input sequence starting with the same stream of tokens?  Shouldn't there be only one &lt;SOS&gt; token?\n",
        "</div>\n",
        "\n",
        "Well, the reason is, we set the tokenizer above to use the &lt;EOS&gt; token as its padding token instead, with the following line:\n",
        "\n",
        "`tokenizer.pad_token = tokenizer.eos_token`\n",
        "\n",
        "We also set `padding_side=\"left\"` earlier when we instantiated the tokenizer.  This was to make it causal (so it can't peek at the next word).\n",
        "\n",
        "As a result, everything is left-padded instead of right, and it's padded with the &lt;EOS&gt; token.  Note that the attention mask is smart and knows not to attend do these tokens (`mask=0`) and the mask is only set to 1 when we want it to attend to the data."
      ],
      "metadata": {
        "id": "xq8cOexiXsCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's drill down into this once more, and look at only a single example (the first) from a single batch, and we'll also print out the decoded values by going back through the tokenizer to decode the IDs:"
      ],
      "metadata": {
        "id": "wTiQxb2LXsCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's examine the first batched example from the dataset\n",
        "for input_batch, label_batch in tfds.take(1):\n",
        "    print(\"Input IDs:\", input_batch['input_ids'][0])\n",
        "    print(tokenizer.batch_decode(input_batch['input_ids'][0]))\n",
        "    print(\"Attention Mask:\", input_batch['attention_mask'][0])\n",
        "    print(\"Label:\", label_batch)\n",
        "    print(\"=\" * 50)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:36.396875Z",
          "iopub.execute_input": "2024-02-01T20:38:36.39794Z",
          "iopub.status.idle": "2024-02-01T20:38:36.444635Z",
          "shell.execute_reply.started": "2024-02-01T20:38:36.397913Z",
          "shell.execute_reply": "2024-02-01T20:38:36.443793Z"
        },
        "trusted": true,
        "id": "IhzQJNGYXsCn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we said above, we can now see that the &lt;EOS&gt; token is used to causally prepend padding on the sequences.  We can also see that our guess of 1 token to 1 word was a good approximation, with only a few words being broken down into multiple tokens, like `Collect` + `ed` or ` Spe` + `eches`."
      ],
      "metadata": {
        "id": "En9idkUVXsCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the Dataset\n",
        "\n",
        "We'll use 10% for a validation dataset just to fairly evaluate how training is going.  We won't do much hyperparameter tuning, just tweaking the learning rate (which needs to be much lower for language modeling, like by a factor of 100x smaller)."
      ],
      "metadata": {
        "id": "rKgNYLIgXsCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we now have a TFDS prepared, let's split it,\n",
        "# to 10% validation set and 90% training\n",
        "\n",
        "# Example calculation:\n",
        "# num_samples = 15_000  # per star rating, so multiplied by 5 star ratings.\n",
        "# (NOTE: this isn't actual num_samples!  Just example to show the math.)\n",
        "# batch_size = 32\n",
        "# num_batches = num_samples * 5 / batch_size = 15,000 * 5 / 32 = int(2343.75) = 2344\n",
        "\n",
        "num_batches = int(num_samples * 5 / batch_size)\n",
        "print(f\"{num_batches} batches of {batch_size} samples each batch.\")\n",
        "\n",
        "# Calculate the size of the training set (90%)\n",
        "train_size = int(0.9 * num_batches)\n",
        "\n",
        "# Split the dataset into training and val sets\n",
        "train_tfds = tfds.take(train_size)\n",
        "val_tfds = tfds.skip(train_size)\n",
        "\n",
        "# Print the sizes of the training and test sets\n",
        "print(f\"Training set size: {train_size}\")\n",
        "print(f\"Validation set size: {num_batches - train_size}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:36.445815Z",
          "iopub.execute_input": "2024-02-01T20:38:36.446958Z",
          "iopub.status.idle": "2024-02-01T20:38:36.458608Z",
          "shell.execute_reply.started": "2024-02-01T20:38:36.446922Z",
          "shell.execute_reply": "2024-02-01T20:38:36.457754Z"
        },
        "trusted": true,
        "id": "7BfamKjvXsCo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Setup and Compilation\n",
        "\n",
        "We'll use a Learning Rate scheduler to slowly anneal the learning rate down to 0 at the very end."
      ],
      "metadata": {
        "id": "Un9GfWCRXsCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to define epochs here, because determines num_training_steps, which\n",
        "# determines the LR scheduler.\n",
        "num_epochs = 5\n",
        "print(f\"Epochs: {num_epochs}\")\n",
        "\n",
        "# The number of training steps is the number of total samples in the dataset,\n",
        "# divided by the batch size, then multiplied by the total number of epochs.\n",
        "# Note that the TF dataset here is a batched tf.data.Dataset,\n",
        "# so its len() is already num_samples // batch_size.\n",
        "num_train_steps = train_size * num_epochs\n",
        "print(f\"Training steps: {num_train_steps}\")\n",
        "\n",
        "# Called \"polynomial,\" but in this basic form with default options, the learning rate will\n",
        "# actually decay linearly.  As mentioned above, we need a much lower initial learning rate\n",
        "# for language modeling tasks than for other ML tasks, partly becuase there are so many\n",
        "# parameters, but also since it's already pre-trained and we're just fine-tuning, avoid\n",
        "# catastrophic forgetting.\n",
        "lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
        ")\n",
        "\n",
        "# language models need much smaller LR than defaults, and we'll use our schedule object here\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\n",
        "\n",
        "# Most HuggingFace models don't SoftMax at output!  Direct from logits\n",
        "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Most HuggingFace models will automatically figure out the loss\n",
        "# (typically what I've picked here), but can still set it ==> Update: bad idea!\n",
        "model.compile(optimizer=opt,\n",
        "              # NOTE:  This was my issue with training!\n",
        "              # Don't specify my own loss, model now trains, see below text for explanation.\n",
        "              #loss=loss,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:36.459672Z",
          "iopub.execute_input": "2024-02-01T20:38:36.460058Z",
          "iopub.status.idle": "2024-02-01T20:38:36.509168Z",
          "shell.execute_reply.started": "2024-02-01T20:38:36.460017Z",
          "shell.execute_reply": "2024-02-01T20:38:36.50811Z"
        },
        "trusted": true,
        "id": "IoiSWfvJXsCo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🐛 Solved a Bug with the Training!\n",
        "\n",
        "When initially training this model, I was having issues getting it to learn anything.  The weird thing is, the loss was decreasing consistently during training, but then when trying to use the model to generate new sentences, it would only output blank spaces.\n",
        "\n",
        "I was kinda at a \"loss\" (ahem) for why my model wasn't learning.  My lucky break was, I tried to fix the code to suppress a warning I was getting (usually a good idea).  The warning was:\n",
        "\n",
        "> \"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input’s attention_mask to obtain reliable results.\"\n",
        "\n",
        "As it turns out, that warning is fine and was a red herring.  But digging into where this error came from led me to reading through the source code for the model and realizing that **the root cause of my issue was with with specifying my own loss function**.\n",
        "\n",
        "I was able to find that this model uses the `TFCausalLanguageModelingLoss` as the loss function.  It is indeed based on the Keras loss `SparseCategoricalCrossentropy(from_logits=True)`, which is what I was using, but it does some special masking to avoid taking certain tokens (those with label `-100`) into account during calculation of the loss.  These special `-100` labels are used as padding tokens during training (since this is a causal model).  Thus they need to be ignored, or masked out, when calculating the loss.  When I specified my own loss function, I picked the right loss equation, but didn't handle these `-100` tokens, hence why my model failed to train!\n",
        "\n",
        "<div class=\"alert alert-warning\">\n",
        "  <strong>\n",
        "      ⚠ Though you <i>can</i> set your own loss function with Hugging Face models, based on this experience, I wouldn't recommend it.  The loss for each model is already set to be what it should, and there is a great chance of missing some loss customization like what happened here, with essentially nothing to gain.  I was just trying to \"test my understanding\" by setting the loss manally, but ended up losing half a day.. though I guess in the end I did increase my understanding! 😁\n",
        "  </strong>\n",
        "</div>"
      ],
      "metadata": {
        "id": "JEhYFDxRXsCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Can read the config of the model, but can also just get from the JSON info online\n",
        "# by visiting https://huggingface.co/distilgpt2/resolve/main/config.json,\n",
        "# which is found in the Transformers library source code.\n",
        "\n",
        "# from transformers import AutoConfig\n",
        "# config = AutoConfig.from_pretrained(checkpoint)\n",
        "# # Access the configuration to get information about the loss function used\n",
        "# print(config)\n",
        "\n",
        "# Actual loss I want is here in the code:\n",
        "# https://huggingface.co/transformers/v3.3.1/_modules/transformers/modeling_tf_utils.html#TFCausalLanguageModelingLoss"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:36.510838Z",
          "iopub.execute_input": "2024-02-01T20:38:36.511249Z",
          "iopub.status.idle": "2024-02-01T20:38:36.517965Z",
          "shell.execute_reply.started": "2024-02-01T20:38:36.511212Z",
          "shell.execute_reply": "2024-02-01T20:38:36.516751Z"
        },
        "trusted": true,
        "id": "aZ5X_xeJXsCo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at what the model looks like.."
      ],
      "metadata": {
        "id": "99YX6RAnXsCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:36.519186Z",
          "iopub.execute_input": "2024-02-01T20:38:36.519581Z",
          "iopub.status.idle": "2024-02-01T20:38:36.5562Z",
          "shell.execute_reply.started": "2024-02-01T20:38:36.519547Z",
          "shell.execute_reply": "2024-02-01T20:38:36.555094Z"
        },
        "trusted": true,
        "id": "8duWfaKhXsCo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.2M parameters, as expected!  Better make sure we use a GPU."
      ],
      "metadata": {
        "id": "lVlw2_ECXsCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU availability\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:36.557626Z",
          "iopub.execute_input": "2024-02-01T20:38:36.558013Z",
          "iopub.status.idle": "2024-02-01T20:38:36.563788Z",
          "shell.execute_reply.started": "2024-02-01T20:38:36.557967Z",
          "shell.execute_reply": "2024-02-01T20:38:36.562553Z"
        },
        "trusted": true,
        "id": "bNz5Ky65XsCp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model\n",
        "\n",
        "We're ready to fit!  Let's just add a custom callback so we can see the learning rate decrease.  We are going for 5 epochs and decreasing linearly, starting from a learning rate of 5e-5, so that works out very nicely to make sure it's decreasing correctly."
      ],
      "metadata": {
        "id": "076jHYsMXsCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom callback to see the learning rate and make sure it's shrinking\n",
        "class PrintLearningRateCB(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
        "        print(f'Epoch {epoch + 1} - Learning Rate: {lr}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:36.564741Z",
          "iopub.execute_input": "2024-02-01T20:38:36.565092Z",
          "iopub.status.idle": "2024-02-01T20:38:36.573628Z",
          "shell.execute_reply.started": "2024-02-01T20:38:36.56506Z",
          "shell.execute_reply": "2024-02-01T20:38:36.572539Z"
        },
        "trusted": true,
        "id": "sDg5xZWmXsCp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell takes about 30 minutes on a P100 GPU."
      ],
      "metadata": {
        "id": "5bjeI9D3XsCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit!\n",
        "model.fit(train_tfds,\n",
        "          validation_data=val_tfds,\n",
        "          epochs=num_epochs,\n",
        "          callbacks=[PrintLearningRateCB()],\n",
        "         )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T20:38:36.574887Z",
          "iopub.execute_input": "2024-02-01T20:38:36.575197Z",
          "iopub.status.idle": "2024-02-01T21:07:41.833469Z",
          "shell.execute_reply.started": "2024-02-01T20:38:36.575174Z",
          "shell.execute_reply": "2024-02-01T21:07:41.832548Z"
        },
        "trusted": true,
        "id": "V2Vkkr_EXsCp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try generating some fake reviews with our trained model!"
      ],
      "metadata": {
        "id": "BXcgad3nXsCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating New Text\n",
        "\n",
        "There are many algorithms we can use to generate new text, and I found [this blog post](https://huggingface.co/blog/how-to-generate) by Hugging Face to be a good refresher.  But don't worry, I'll walk through the various algorithms here as we go."
      ],
      "metadata": {
        "id": "qftqSAaJXsCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Greedy Search\n",
        "\n",
        "Greedy Search is a text generation where, given an input prompt (or \"context\"), we literally just pick the next most likely word at that moment, given what we've seen already.  Your phone's keyboard does a rough version of this when you are typing, by suggesting the next most likely word.  This is often based on an older method (before deep learning), using n-grams and other statistical methods.  \n",
        "\n",
        "Though greedy search is simple, it often ends up creating very repetitive text."
      ],
      "metadata": {
        "id": "q7qzpiqvXsCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use these same prompts throughout evaluation\n",
        "prompt1 = \"A 5-star review of the book \\\"Words to Comfort, Words to Heal\\\": \"\n",
        "prompt2 = \"A 1-star review of the book \\\"The Evolution of Useful Things\\\": \"\n",
        "prompt3 = \"A 3-star review of the book \\\"Crossing the Chasm\\\": \""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T21:10:11.674386Z",
          "iopub.execute_input": "2024-02-01T21:10:11.674788Z",
          "iopub.status.idle": "2024-02-01T21:10:11.679802Z",
          "shell.execute_reply.started": "2024-02-01T21:10:11.674757Z",
          "shell.execute_reply": "2024-02-01T21:10:11.678735Z"
        },
        "trusted": true,
        "id": "4RTQSUm9XsCq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: Tokenizer takes a list of input prompts, like [prompt1].\n",
        "# But if generating more than one prompt at a time, there will be an error because\n",
        "# not all the input strings are the same length!  Tokenizer cannot convert them to tensors.\n",
        "# To resolve this, use padding and truncation arguments of the tokenizer to make\n",
        "# the input sequences all the same length.\n",
        "encodings = tokenizer([prompt1, prompt2, prompt3],\n",
        "                      return_tensors='tf',\n",
        "                      padding=True,\n",
        "                      truncation=True\n",
        "                     )\n",
        "\n",
        "# Use the newly trained model to generate new outputs\n",
        "# Use max_new_tokens=max_length (which is 64 here), so outputs are no longer than the sequences they\n",
        "# were trained on.\n",
        "outputs = model.generate(**encodings,\n",
        "                         max_new_tokens=max_length, # not including the input prompts\n",
        "                         pad_token_id=tokenizer.eos_token_id,\n",
        "                        )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T21:10:16.199678Z",
          "iopub.execute_input": "2024-02-01T21:10:16.200066Z",
          "iopub.status.idle": "2024-02-01T21:10:23.816832Z",
          "shell.execute_reply.started": "2024-02-01T21:10:16.200036Z",
          "shell.execute_reply": "2024-02-01T21:10:23.816035Z"
        },
        "trusted": true,
        "id": "IrxbqYhQXsCq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just once, let's look at the output tokens generated by the model:"
      ],
      "metadata": {
        "id": "-v2o8wUaXsCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T21:10:28.385954Z",
          "iopub.execute_input": "2024-02-01T21:10:28.386337Z",
          "iopub.status.idle": "2024-02-01T21:10:28.392725Z",
          "shell.execute_reply.started": "2024-02-01T21:10:28.386304Z",
          "shell.execute_reply": "2024-02-01T21:10:28.391712Z"
        },
        "trusted": true,
        "id": "Sx1doFXgXsCq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, outputs is a tensor with shape (3, 82); one output corresponding to each input prompt.  The output length also includes the input prompts.  We can see also the outputs are padded with the &lt;EOS&gt; special token at the end (see especially last example).  \n",
        "\n",
        "But let's have the tokenizer decode these outputs so they are human-readable:"
      ],
      "metadata": {
        "id": "OzxH6_MOXsCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode those generated token ids back to text\n",
        "# Don't print out <SOS>, <EOS>, padding, etc.\n",
        "decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# Easier to read if we unroll the list\n",
        "for out in decoded:\n",
        "    print(f\"{out}\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T21:11:00.511379Z",
          "iopub.execute_input": "2024-02-01T21:11:00.512238Z",
          "iopub.status.idle": "2024-02-01T21:11:00.52175Z",
          "shell.execute_reply.started": "2024-02-01T21:11:00.512193Z",
          "shell.execute_reply": "2024-02-01T21:11:00.520672Z"
        },
        "trusted": true,
        "id": "ZHbVj1UcXsCq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how these examples tend to repeat themselves.  \n",
        "\n",
        "Also, even if we didn't do any further optimizations (and even though the results aren't great), these results are *MUCH* better than the original response of the non-fine-tuned model.  To jog your memory, it was:\n",
        "\n",
        "> 'A 1-star review of the book \"The Evolution of Useful Things\": ______________________________________ / The book \"The Evolution of Useful Things\": ______________________________________ / The book \"The Evolution of Useful Things\": ______________________________________ / The book \"The Evolution of Useful Things\": ______________________________________ / The book \"The Evolution of'"
      ],
      "metadata": {
        "id": "KHaSUPpzXsCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beam Search\n",
        "\n",
        "Instead of finding the sequence one word at a time, which often isn't the optimal result, beam search allows for sortof a parallel search of candidate beams.  As each new word is generated, all beams are evaluated based on which is most likely (often using a metric such as **perplexity**).  Beam search will always outperform greedy search, but it isn't guaranteed to find the best overall \"most likely output\" phrase (that is, the one with lowest perplexity)."
      ],
      "metadata": {
        "id": "Bzxq4g5mXsCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We don't need to generate encodings again with the tokenizer, since\n",
        "# we're keeping the prompts the same!\n",
        "\n",
        "outputs = model.generate(**encodings,\n",
        "                         max_new_tokens=max_length, # not including prompts\n",
        "                         num_beams=5,               # beam search on\n",
        "                         do_sample=True,            # use sample probabilities for next word\n",
        "                         num_return_sequences=1,    # could return more, just see best\n",
        "                         pad_token_id=tokenizer.eos_token_id,\n",
        "                         # Stop generation when all beam hypotheses reach the EOS token.\n",
        "                         early_stopping=True,\n",
        "                        )\n",
        "\n",
        "decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# Easier to read if we unroll the list\n",
        "for out in decoded:\n",
        "    print(f\"{out}\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T21:17:19.518221Z",
          "iopub.execute_input": "2024-02-01T21:17:19.518596Z",
          "iopub.status.idle": "2024-02-01T21:17:28.522451Z",
          "shell.execute_reply.started": "2024-02-01T21:17:19.518566Z",
          "shell.execute_reply": "2024-02-01T21:17:28.521419Z"
        },
        "trusted": true,
        "id": "mWVhv7KtXsCr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hmm, not so good, it basically didn't generate a review for the first two.\n",
        "\n",
        "Beam search works well for structured tasks like machine translation or summarization, where the output is constrained by what the input was (you'd also use an Encoder-Decoder transformer architecture for this task, not a Decoder-only architecture, like GPTs).  But when doing open-ended text generation, there is no structure to guide the output, so even though we're searching with many beams, the lengths can vary greatly and results are mixed, as seen here."
      ],
      "metadata": {
        "id": "9L3CcQ83XsCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## n-gram Penalties\n",
        "\n",
        "Another strategy is to penalize repeating **n-grams** (groups of *n* tokens).  With no n-grams set to 2, for example, it means it will never repeat the same 2-word group twice.  So if you wrote an article about the \"United States,\" it could only say \"United States\" once in the generated text (this ignores occurrences in the prompt itself, and just limits the output), because once it predicted that once, then next time it predicts \"United,\" it is forbidden from predicting \"States\" again!  This is quite limiting, but does result in less repetition."
      ],
      "metadata": {
        "id": "MYAEGsDxXsCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We don't need to generate encodings again with the tokenizer, since\n",
        "# we're keeping the prompts the same!\n",
        "\n",
        "outputs = model.generate(**encodings,\n",
        "                         max_new_tokens=max_length, # not including prompts\n",
        "                         num_beams=5,               # beam search on\n",
        "                         do_sample=True,            # use sample probabilities for next word\n",
        "                         num_return_sequences=1,    # could return more, just see best\n",
        "                         pad_token_id=tokenizer.eos_token_id,\n",
        "                         # Stop generation when all beam hypotheses reach the EOS token.\n",
        "                         early_stopping=True,\n",
        "                         # Make no 2-gram appear twice, to try to reduce repetitions\n",
        "                         no_repeat_ngram_size=2,\n",
        "                        )\n",
        "\n",
        "decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# Easier to read if we unroll the list\n",
        "for out in decoded:\n",
        "    print(f\"{out}\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T21:19:28.004926Z",
          "iopub.execute_input": "2024-02-01T21:19:28.005273Z",
          "iopub.status.idle": "2024-02-01T21:19:55.208616Z",
          "shell.execute_reply.started": "2024-02-01T21:19:28.005248Z",
          "shell.execute_reply": "2024-02-01T21:19:55.20772Z"
        },
        "trusted": true,
        "id": "iJu5S7u9XsCr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "These results seem the best so far.\n",
        "\n",
        "My favorite review so far:\n",
        "\n",
        "> This book is a waste of time, money, and time.  If you're not a vegetarian, this is not for you!  Don't waste your time or your money on this book!  DON'T waste any money!\n",
        "\n",
        "You can't *buy* this review gold on Mechanical Turk! 😎"
      ],
      "metadata": {
        "id": "oD7R5KATXsCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling\n",
        "\n",
        "Beam search (and Greedy search of course) look for high probability distributions.  But studies have shown that real human language does not follow high-probability distributions, and so text that does will sound very robotic.\n",
        "\n",
        "So we can make it sound more \"real\" and less predictable by introducing some randomness with **sampling**.  Sampling is, rather than always picking the word with the highest probability, choosing the next word relative to the probability distribution.  Thus there is still some random chance to choosing the next word, and we can make it sound less repetitive.\n",
        "\n",
        "We've actually already turned on sampling earlier when we covered beam search, but we'll refine our sampling now.  In a sense, every word in the output vocabulary is fair game with this next spin, which should lead to quite random looking text."
      ],
      "metadata": {
        "id": "BMj6X3qfXsCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We don't need to generate encodings again with the tokenizer, since\n",
        "# we're keeping the prompts the same!\n",
        "\n",
        "outputs = model.generate(**encodings,\n",
        "                         max_new_tokens=max_length, # not including prompts\n",
        "                         do_sample=True,            # sample probabilities for next word\n",
        "                         pad_token_id=tokenizer.eos_token_id,\n",
        "                        )\n",
        "\n",
        "decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# Easier to read if we unroll the list\n",
        "for out in decoded:\n",
        "    print(f\"{out}\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T21:32:37.610503Z",
          "iopub.execute_input": "2024-02-01T21:32:37.611073Z",
          "iopub.status.idle": "2024-02-01T21:32:44.379804Z",
          "shell.execute_reply.started": "2024-02-01T21:32:37.61104Z",
          "shell.execute_reply": "2024-02-01T21:32:44.378843Z"
        },
        "trusted": true,
        "id": "2Bef8rXaXsCr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Incoherent gibberish!  The words follow basic English grammar rules, but they don't really make any sense when you start reading them.\n",
        "\n",
        "## Tweak the Temperature\n",
        "\n",
        "One way to try to mitigate this is to play with the **temperature**, which is a knob we have to tweak when doing the random sampling.  Temperature defaults to 1 (as in the above case), and when we use a low temperature (T < 1), we sharpen the probability distribution of predicted next words, such that we increase the likelihood of higher probability words and decrease the probability of lower probability words.  Similarly, we could *increase* the temperature (T > 1), which would do the opposite, and make words even more random seeming.  But they're already bad enough!\n",
        "\n",
        "Let's try lowering the temperature to 0.6 and see what that change alone does to the generated outputs."
      ],
      "metadata": {
        "id": "B2Ezum9oXsCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We don't need to generate encodings again with the tokenizer, since\n",
        "# we're keeping the prompts the same!\n",
        "\n",
        "outputs = model.generate(**encodings,\n",
        "                         max_new_tokens=max_length, # not including prompts\n",
        "                         do_sample=True,            # sample probabilities for next word\n",
        "                         pad_token_id=tokenizer.eos_token_id,\n",
        "                         # Sharpen the probability curve (higher chance for more common words,\n",
        "                         # lower chance for less common words)\n",
        "                         temperature=0.6,\n",
        "                        )\n",
        "\n",
        "decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# Easier to read if we unroll the list\n",
        "for out in decoded:\n",
        "    print(f\"{out}\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T21:38:42.048007Z",
          "iopub.execute_input": "2024-02-01T21:38:42.048398Z",
          "iopub.status.idle": "2024-02-01T21:38:48.759538Z",
          "shell.execute_reply.started": "2024-02-01T21:38:42.048368Z",
          "shell.execute_reply": "2024-02-01T21:38:48.758651Z"
        },
        "trusted": true,
        "id": "_T48kGCGXsCr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "That definitely looks better than with temperature=1 (which was basically random words with a correct-looking structure), but it's still not great, so let's try some other strategies.\n",
        "\n",
        "Note that, as temperature would decrease to 0, it becomes equal to greedy decoding and has the same problems that greedy decoding has."
      ],
      "metadata": {
        "id": "EV6fJd5yXsCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top-K Sampling\n",
        "\n",
        "**Top-K Sampling** essentially just takes the top *K* words from the entire probability distribution, and redistributes the probability distribution amongst those.  For example, if you looked at the Top-10 words at each step, then the probability of those words would add up to 1.0 (whereas before the redistribution, the only way to sum the probability distribution to 1.0 would be to include ALL words in the vocab).  This eliminates unlikely next words, but keeps some sampling in effect."
      ],
      "metadata": {
        "id": "xTp0Sk_AXsCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We don't need to generate encodings again with the tokenizer, since\n",
        "# we're keeping the prompts the same!\n",
        "\n",
        "outputs = model.generate(**encodings,\n",
        "                         max_new_tokens=max_length,  # not including prompts\n",
        "                         do_sample=True,             # sample probabilities for next word\n",
        "                         pad_token_id=tokenizer.eos_token_id,\n",
        "                         top_k=50,                   # Top-50 words (or tokens, technically)\n",
        "                        )\n",
        "\n",
        "decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# Easier to read if we unroll the list\n",
        "for out in decoded:\n",
        "    print(f\"{out}\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T21:42:26.190799Z",
          "iopub.execute_input": "2024-02-01T21:42:26.191295Z",
          "iopub.status.idle": "2024-02-01T21:42:33.080736Z",
          "shell.execute_reply.started": "2024-02-01T21:42:26.191253Z",
          "shell.execute_reply": "2024-02-01T21:42:33.079753Z"
        },
        "trusted": true,
        "id": "_V3cVzq0XsCr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Top-P (Nucleus) Sampling\n",
        "\n",
        "One issue with Top-K is that the K number is fixed, so if our next word distribution was pretty-flat (any of those words are similarly likely) or was very sharp (some words much more likely), it'd all get lumped into the same K.  So instead of sampling from most likely K words, **Top-P Sampling** (or nucleus sampling) chooses instead a *probability* to be fixed.  In other words, it's a fixed probability instead of a fixed top-word count.  For example, if we wanted to only pick from the words that represent the top 95% of words, we could do that.  \n",
        "\n",
        "We can use this by setting a top_p setting between 0 and 1 (note that top_k defaults to 0 if we don't set it).  Note also that top_p defaults to 1.0, and in practice, most LLMs use a top_p somewhere between 0.90-0.95."
      ],
      "metadata": {
        "id": "uIc2TDzSXsCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We don't need to generate encodings again with the tokenizer, since\n",
        "# we're keeping the prompts the same!\n",
        "\n",
        "outputs = model.generate(**encodings,\n",
        "                         max_new_tokens=max_length, # not including prompts\n",
        "                         do_sample=True,            # sample probabilities for next word\n",
        "                         pad_token_id=tokenizer.eos_token_id,\n",
        "                         top_p=0.92,                # Use Top-P Nucleus sampling instead\n",
        "                        )\n",
        "\n",
        "decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# Easier to read if we unroll the list\n",
        "for out in decoded:\n",
        "    print(f\"{out}\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T21:51:14.86572Z",
          "iopub.execute_input": "2024-02-01T21:51:14.866588Z",
          "iopub.status.idle": "2024-02-01T21:51:22.231571Z",
          "shell.execute_reply.started": "2024-02-01T21:51:14.866554Z",
          "shell.execute_reply": "2024-02-01T21:51:22.230666Z"
        },
        "trusted": true,
        "id": "zYRF-QnFXsCs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting It All Together\n",
        "\n",
        "Results are still rather bizarre.  Also, one thing to notice is that the longer the generated sequences, the more random things the LLM is going to bring up.  Perhaps we can get a more realistic sounding fake review by forcing the model to be more terse?\n",
        "\n",
        "It also tends to generate all the way or almost all the way up to the max length, but most of the training data was much shorter.  In fact, if you look a the histogram we created earlier, we see a peak of most training examples taking 20 tokens.  This *includes* even the prompt in that generation, so we could even do shorter since the `max_new_tokens` doesn't include the prompt.  But lets not overconstrain it and include both sides of the bell curve, and set `max_new_tokens=32` instead.\n",
        "\n",
        "We can even combine methods, like turning beam search back on, and can even run top_k and top_p at the same time.  The Hugging Face model will perform top-k sampling, and then top-p sampling within that.  We can turn on n-gram avoidance to help cut down repetitions.  We can also set an `early_stopping` flag, which will stop generation when all beam hypotheses reach the &lt;EOS&gt; token."
      ],
      "metadata": {
        "id": "RxeUgBrGXsCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We don't need to generate encodings again with the tokenizer, since\n",
        "# we're keeping the prompts the same!\n",
        "\n",
        "outputs = model.generate(**encodings,\n",
        "                         max_new_tokens=32,         # not including prompts\n",
        "                         do_sample=True,            # sample probabilities for next word\n",
        "                         pad_token_id=tokenizer.eos_token_id,\n",
        "                         top_k=250,\n",
        "                         # Use Top-P sampling.  As approaches 1, the more all words are included!\n",
        "                         top_p=0.92,\n",
        "                         no_repeat_ngram_size=3,    # Make no 3-gram appear twice, reduce rep.\n",
        "                         num_beams=5,               # Beam search on\n",
        "                         num_return_sequences=1,    # could return more, just see best\n",
        "                         # Stop generation when all beam hypotheses reach the EOS token\n",
        "                         early_stopping=True,\n",
        "                        )\n",
        "\n",
        "decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "# Easier to read if we unroll the list\n",
        "for out in decoded:\n",
        "    print(f\"{out}\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-01T22:02:19.099741Z",
          "iopub.execute_input": "2024-02-01T22:02:19.100112Z",
          "iopub.status.idle": "2024-02-01T22:02:32.818478Z",
          "shell.execute_reply.started": "2024-02-01T22:02:19.100085Z",
          "shell.execute_reply": "2024-02-01T22:02:32.817508Z"
        },
        "trusted": true,
        "id": "4JOcX7uPXsCs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the time I ran this notebook, the final output was:\n",
        "\n",
        "> A 5-star review of the book \"Words to Comfort, Words to Heal\":  This book is a must-have for any self-medicants, not just for those with ADD or ADHD, but for anyone with ADD/ADHD  \n",
        "> A 1-star review of the book \"The Evolution of Useful Things\":  This book is a waste of time, time, and money, and a lot of time and money.  This is not a good book for anyone to read  \n",
        "> A 3-star review of the book \"Crossing the Chasm\":  This is a good book, but I'm not sure I'll ever be able to finish it again.    If you're going to read this,  \n",
        "\n",
        "At another time I ran it, the output was:\n",
        "\n",
        "> A 5-star review of the book \"Words to Comfort, Words to Heal\":  It's a wonderful way to help a loved one understand and understand the meaning of words to comfort, words to heal, and words to help you cope with pain  \n",
        "> A 1-star review of the book \"The Evolution of Useful Things\":  It's not what I expected, but I didn't get it.  I'm not sure what I was expecting.    I was disappointed.  \n",
        "> A 3-star review of the book \"Crossing the Chasm\":  A good read, but it's not what you'd expect for a good book on the subject, and it's a little too much of a read for a  \n",
        "\n",
        "👍 Not too bad!  You could now write a script to iterate a million times and generate a million sorta-decent fake reviews.  But we won't do that because we're good people, so let's give Jeff a call to show him our results."
      ],
      "metadata": {
        "id": "jk2vhztPXsCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "\"Hey Jeff, this is Joel.  I've got those results we talked about, and they look pretty good.  I think you could use them to gener..\n",
        "\n",
        "> Oh yeah, heya buddy.  So, it turns out, I'm not gonna be needing that anymore.. so..\n",
        "\n",
        "What do you mean?!  I just spent a bunch of time doing all this..\n",
        "\n",
        "> Yeah, I understand.  So I just got off the phone with Andy and it sounds like Elon's got the same problem with bots and fake tweets over on X, so, I guess it's not really a problem for us, either.\n",
        "\n",
        "What! You don't have to have everything that Elon..\n",
        "\n",
        "![8eeddx.jpg](attachment:26236c69-1e5f-409a-bd8f-902b3b6bacb5.jpg)"
      ],
      "metadata": {
        "id": "mwUPkKZ3XsCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll keep my eyes peeled for that offer letter from Amazon in my inbox.  😉"
      ],
      "metadata": {
        "id": "GOZa4d_5XsCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One last thing..\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "    If you’ve enjoyed following along with me or have learned something new, ☝ <b>please consider UPVOTING this notebook!</b> 🔼 It helps others discover my notebook and encourages me to spend my time writing more of these.  And follow me here on Kaggle to read more notebooks like this and embark on a journey together towards AI/ML knowledge! 😊\n",
        "</div>"
      ],
      "metadata": {
        "id": "rxuAdVa-XsCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "💡 **You might also enjoy these other notebooks of mine as well:**\n",
        "\n",
        "**Natural Language Processing**\n",
        "* [Generate Amazon Book Reviews with Transformers](https://www.kaggle.com/code/quackaddict7/generate-amazon-book-reviews-with-transformers/)  \n",
        "* [Write Your Own CliffNotes (Book Text Summarizer)](https://www.kaggle.com/code/quackaddict7/write-your-own-cliffnotes-book-text-summarizer/)  \n",
        "* [Answering Questions from Product Reviews](https://www.kaggle.com/code/quackaddict7/answering-questions-from-product-reviews/)  \n",
        "* [Ingredient Standardization via Machine Translation](https://www.kaggle.com/code/quackaddict7/ingredient-standardization-via-machine-translation/)  \n",
        "* [What Kaggle WON'T Tell You About Your Notebooks](https://www.kaggle.com/code/quackaddict7/what-kaggle-won-t-tell-you-about-your-notebooks/)  \n",
        "\n",
        "**Computer Vision**\n",
        "* [Enhance, enhance, enhance! (image upscaling)](https://www.kaggle.com/code/quackaddict7/enhance-enhance-enhance-image-upscaling)  \n",
        "* [Detecting Vehicles in Traffic (Object Detection)](https://www.kaggle.com/code/quackaddict7/object-detection-detecting-vehicles-in-traffic/)  \n",
        "* [Legions of Lesions (Detecting Skin Cancer with Computer Vision)](https://www.kaggle.com/code/quackaddict7/legions-of-lesions-detecting-skin-cancer-with-cv/)  \n",
        "* [Creating Synthetic Wildfire Images with Unreal Engine (Blog Post)](https://joelwigton.com/synthetic-data-for-machine-learning-with-unreal-engine)  \n",
        "* [How I scored Top 5% on MNIST (without cheating!)](https://www.kaggle.com/code/quackaddict7/how-i-scored-top-5-on-mnist-without-cheating/)  \n",
        "\n",
        "**Machine Learning**\n",
        "* [Detecting Android Malware from App Permissions](https://www.kaggle.com/code/quackaddict7/detecting-android-malware-from-app-permissions/)  \n"
      ],
      "metadata": {
        "id": "Rx5TmvI9XsCs"
      }
    }
  ]
}